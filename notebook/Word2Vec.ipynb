{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75c2963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import gensim.downloader\n",
    "from gensim.models import Word2Vec\n",
    "import scipy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa5890b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = gensim.downloader.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43968231",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('twitter_cleaned.csv')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d3712ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweet = df['message'].apply(lambda x: x.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f4eb5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 8min 22s\n",
      "Wall time: 1min 23s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(14292250, 19171820)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = Word2Vec(\n",
    "            tokenized_tweet,\n",
    "            vector_size=200, # desired no. of features/independent variables\n",
    "            window=5, # context window size\n",
    "            min_count=2, # Ignores all words with total frequency lower than 2.                                  \n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 32, # no.of cores\n",
    "            seed = 34\n",
    ") \n",
    "\n",
    "model.train(tokenized_tweet, total_examples= len(df['message']), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48f1b4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model.wv[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:  # handling the case where the token is not in vocabulary\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec3be906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49675, 200)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec_arrays = np.zeros((len(tokenized_tweet), 200)) \n",
    "for i in range(len(tokenized_tweet)):\n",
    "    try:\n",
    "            wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 200)\n",
    "    except KeyError:  # handling the case where the token is not in vocabulary\n",
    "            continue\n",
    "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "wordvec_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0b11a62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.021858</td>\n",
       "      <td>-0.136008</td>\n",
       "      <td>-0.079867</td>\n",
       "      <td>0.142262</td>\n",
       "      <td>-0.334975</td>\n",
       "      <td>0.006666</td>\n",
       "      <td>-0.085440</td>\n",
       "      <td>0.008387</td>\n",
       "      <td>0.076322</td>\n",
       "      <td>-0.042974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267254</td>\n",
       "      <td>-0.212343</td>\n",
       "      <td>0.075734</td>\n",
       "      <td>0.067654</td>\n",
       "      <td>0.181160</td>\n",
       "      <td>-0.150075</td>\n",
       "      <td>-0.048985</td>\n",
       "      <td>-0.284484</td>\n",
       "      <td>0.137764</td>\n",
       "      <td>0.179570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.100456</td>\n",
       "      <td>-0.260915</td>\n",
       "      <td>-0.010394</td>\n",
       "      <td>-0.032613</td>\n",
       "      <td>-0.222945</td>\n",
       "      <td>-0.084207</td>\n",
       "      <td>-0.161302</td>\n",
       "      <td>-0.160732</td>\n",
       "      <td>0.091418</td>\n",
       "      <td>-0.080268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126912</td>\n",
       "      <td>-0.262101</td>\n",
       "      <td>0.061075</td>\n",
       "      <td>-0.092836</td>\n",
       "      <td>0.134562</td>\n",
       "      <td>0.088054</td>\n",
       "      <td>-0.052204</td>\n",
       "      <td>-0.246774</td>\n",
       "      <td>0.008476</td>\n",
       "      <td>-0.082413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.108536</td>\n",
       "      <td>-0.243100</td>\n",
       "      <td>0.025253</td>\n",
       "      <td>-0.038388</td>\n",
       "      <td>-0.250112</td>\n",
       "      <td>0.014699</td>\n",
       "      <td>-0.217285</td>\n",
       "      <td>0.007953</td>\n",
       "      <td>0.193621</td>\n",
       "      <td>0.039194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035768</td>\n",
       "      <td>-0.247438</td>\n",
       "      <td>0.149372</td>\n",
       "      <td>0.052816</td>\n",
       "      <td>0.101603</td>\n",
       "      <td>0.206118</td>\n",
       "      <td>-0.017807</td>\n",
       "      <td>-0.195487</td>\n",
       "      <td>0.170776</td>\n",
       "      <td>-0.070792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.019540</td>\n",
       "      <td>-0.273690</td>\n",
       "      <td>-0.107766</td>\n",
       "      <td>0.043199</td>\n",
       "      <td>-0.260629</td>\n",
       "      <td>-0.081694</td>\n",
       "      <td>0.058818</td>\n",
       "      <td>-0.201367</td>\n",
       "      <td>0.114887</td>\n",
       "      <td>-0.027537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125935</td>\n",
       "      <td>-0.374731</td>\n",
       "      <td>-0.008154</td>\n",
       "      <td>-0.048901</td>\n",
       "      <td>0.117185</td>\n",
       "      <td>-0.010782</td>\n",
       "      <td>-0.039707</td>\n",
       "      <td>-0.268269</td>\n",
       "      <td>0.148644</td>\n",
       "      <td>-0.005968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.001681</td>\n",
       "      <td>-0.189922</td>\n",
       "      <td>-0.008858</td>\n",
       "      <td>-0.030504</td>\n",
       "      <td>-0.193903</td>\n",
       "      <td>-0.117119</td>\n",
       "      <td>-0.157301</td>\n",
       "      <td>-0.092671</td>\n",
       "      <td>0.145255</td>\n",
       "      <td>-0.025514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.193059</td>\n",
       "      <td>-0.213469</td>\n",
       "      <td>0.078424</td>\n",
       "      <td>-0.056320</td>\n",
       "      <td>0.095797</td>\n",
       "      <td>0.186037</td>\n",
       "      <td>-0.047434</td>\n",
       "      <td>-0.275176</td>\n",
       "      <td>-0.010689</td>\n",
       "      <td>0.007353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.021858 -0.136008 -0.079867  0.142262 -0.334975  0.006666 -0.085440   \n",
       "1 -0.100456 -0.260915 -0.010394 -0.032613 -0.222945 -0.084207 -0.161302   \n",
       "2  0.108536 -0.243100  0.025253 -0.038388 -0.250112  0.014699 -0.217285   \n",
       "3  0.019540 -0.273690 -0.107766  0.043199 -0.260629 -0.081694  0.058818   \n",
       "4 -0.001681 -0.189922 -0.008858 -0.030504 -0.193903 -0.117119 -0.157301   \n",
       "\n",
       "        7         8         9    ...       190       191       192       193  \\\n",
       "0  0.008387  0.076322 -0.042974  ...  0.267254 -0.212343  0.075734  0.067654   \n",
       "1 -0.160732  0.091418 -0.080268  ...  0.126912 -0.262101  0.061075 -0.092836   \n",
       "2  0.007953  0.193621  0.039194  ...  0.035768 -0.247438  0.149372  0.052816   \n",
       "3 -0.201367  0.114887 -0.027537  ...  0.125935 -0.374731 -0.008154 -0.048901   \n",
       "4 -0.092671  0.145255 -0.025514  ...  0.193059 -0.213469  0.078424 -0.056320   \n",
       "\n",
       "        194       195       196       197       198       199  \n",
       "0  0.181160 -0.150075 -0.048985 -0.284484  0.137764  0.179570  \n",
       "1  0.134562  0.088054 -0.052204 -0.246774  0.008476 -0.082413  \n",
       "2  0.101603  0.206118 -0.017807 -0.195487  0.170776 -0.070792  \n",
       "3  0.117185 -0.010782 -0.039707 -0.268269  0.148644 -0.005968  \n",
       "4  0.095797  0.186037 -0.047434 -0.275176 -0.010689  0.007353  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be5e347e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.021858</td>\n",
       "      <td>-0.136008</td>\n",
       "      <td>-0.079867</td>\n",
       "      <td>0.142262</td>\n",
       "      <td>-0.334975</td>\n",
       "      <td>0.006666</td>\n",
       "      <td>-0.085440</td>\n",
       "      <td>0.008387</td>\n",
       "      <td>0.076322</td>\n",
       "      <td>-0.042974</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267254</td>\n",
       "      <td>-0.212343</td>\n",
       "      <td>0.075734</td>\n",
       "      <td>0.067654</td>\n",
       "      <td>0.181160</td>\n",
       "      <td>-0.150075</td>\n",
       "      <td>-0.048985</td>\n",
       "      <td>-0.284484</td>\n",
       "      <td>0.137764</td>\n",
       "      <td>0.179570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.100456</td>\n",
       "      <td>-0.260915</td>\n",
       "      <td>-0.010394</td>\n",
       "      <td>-0.032613</td>\n",
       "      <td>-0.222945</td>\n",
       "      <td>-0.084207</td>\n",
       "      <td>-0.161302</td>\n",
       "      <td>-0.160732</td>\n",
       "      <td>0.091418</td>\n",
       "      <td>-0.080268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.126912</td>\n",
       "      <td>-0.262101</td>\n",
       "      <td>0.061075</td>\n",
       "      <td>-0.092836</td>\n",
       "      <td>0.134562</td>\n",
       "      <td>0.088054</td>\n",
       "      <td>-0.052204</td>\n",
       "      <td>-0.246774</td>\n",
       "      <td>0.008476</td>\n",
       "      <td>-0.082413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.108536</td>\n",
       "      <td>-0.243100</td>\n",
       "      <td>0.025253</td>\n",
       "      <td>-0.038388</td>\n",
       "      <td>-0.250112</td>\n",
       "      <td>0.014699</td>\n",
       "      <td>-0.217285</td>\n",
       "      <td>0.007953</td>\n",
       "      <td>0.193621</td>\n",
       "      <td>0.039194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035768</td>\n",
       "      <td>-0.247438</td>\n",
       "      <td>0.149372</td>\n",
       "      <td>0.052816</td>\n",
       "      <td>0.101603</td>\n",
       "      <td>0.206118</td>\n",
       "      <td>-0.017807</td>\n",
       "      <td>-0.195487</td>\n",
       "      <td>0.170776</td>\n",
       "      <td>-0.070792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.019540</td>\n",
       "      <td>-0.273690</td>\n",
       "      <td>-0.107766</td>\n",
       "      <td>0.043199</td>\n",
       "      <td>-0.260629</td>\n",
       "      <td>-0.081694</td>\n",
       "      <td>0.058818</td>\n",
       "      <td>-0.201367</td>\n",
       "      <td>0.114887</td>\n",
       "      <td>-0.027537</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125935</td>\n",
       "      <td>-0.374731</td>\n",
       "      <td>-0.008154</td>\n",
       "      <td>-0.048901</td>\n",
       "      <td>0.117185</td>\n",
       "      <td>-0.010782</td>\n",
       "      <td>-0.039707</td>\n",
       "      <td>-0.268269</td>\n",
       "      <td>0.148644</td>\n",
       "      <td>-0.005968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.001681</td>\n",
       "      <td>-0.189922</td>\n",
       "      <td>-0.008858</td>\n",
       "      <td>-0.030504</td>\n",
       "      <td>-0.193903</td>\n",
       "      <td>-0.117119</td>\n",
       "      <td>-0.157301</td>\n",
       "      <td>-0.092671</td>\n",
       "      <td>0.145255</td>\n",
       "      <td>-0.025514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.193059</td>\n",
       "      <td>-0.213469</td>\n",
       "      <td>0.078424</td>\n",
       "      <td>-0.056320</td>\n",
       "      <td>0.095797</td>\n",
       "      <td>0.186037</td>\n",
       "      <td>-0.047434</td>\n",
       "      <td>-0.275176</td>\n",
       "      <td>-0.010689</td>\n",
       "      <td>0.007353</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.021858 -0.136008 -0.079867  0.142262 -0.334975  0.006666 -0.085440   \n",
       "1 -0.100456 -0.260915 -0.010394 -0.032613 -0.222945 -0.084207 -0.161302   \n",
       "2  0.108536 -0.243100  0.025253 -0.038388 -0.250112  0.014699 -0.217285   \n",
       "3  0.019540 -0.273690 -0.107766  0.043199 -0.260629 -0.081694  0.058818   \n",
       "4 -0.001681 -0.189922 -0.008858 -0.030504 -0.193903 -0.117119 -0.157301   \n",
       "\n",
       "        7         8         9    ...       190       191       192       193  \\\n",
       "0  0.008387  0.076322 -0.042974  ...  0.267254 -0.212343  0.075734  0.067654   \n",
       "1 -0.160732  0.091418 -0.080268  ...  0.126912 -0.262101  0.061075 -0.092836   \n",
       "2  0.007953  0.193621  0.039194  ...  0.035768 -0.247438  0.149372  0.052816   \n",
       "3 -0.201367  0.114887 -0.027537  ...  0.125935 -0.374731 -0.008154 -0.048901   \n",
       "4 -0.092671  0.145255 -0.025514  ...  0.193059 -0.213469  0.078424 -0.056320   \n",
       "\n",
       "        194       195       196       197       198       199  \n",
       "0  0.181160 -0.150075 -0.048985 -0.284484  0.137764  0.179570  \n",
       "1  0.134562  0.088054 -0.052204 -0.246774  0.008476 -0.082413  \n",
       "2  0.101603  0.206118 -0.017807 -0.195487  0.170776 -0.070792  \n",
       "3  0.117185 -0.010782 -0.039707 -0.268269  0.148644 -0.005968  \n",
       "4  0.095797  0.186037 -0.047434 -0.275176 -0.010689  0.007353  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordvec_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52b79e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.571994</td>\n",
       "      <td>0.623757</td>\n",
       "      <td>0.309427</td>\n",
       "      <td>0.718099</td>\n",
       "      <td>0.373818</td>\n",
       "      <td>0.508229</td>\n",
       "      <td>0.503443</td>\n",
       "      <td>0.526289</td>\n",
       "      <td>0.426138</td>\n",
       "      <td>0.476286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.647968</td>\n",
       "      <td>0.610293</td>\n",
       "      <td>0.517064</td>\n",
       "      <td>0.644147</td>\n",
       "      <td>0.564423</td>\n",
       "      <td>0.245045</td>\n",
       "      <td>0.510967</td>\n",
       "      <td>0.407351</td>\n",
       "      <td>0.508058</td>\n",
       "      <td>0.647381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.462480</td>\n",
       "      <td>0.471853</td>\n",
       "      <td>0.388536</td>\n",
       "      <td>0.531941</td>\n",
       "      <td>0.497407</td>\n",
       "      <td>0.382798</td>\n",
       "      <td>0.434003</td>\n",
       "      <td>0.361507</td>\n",
       "      <td>0.443034</td>\n",
       "      <td>0.431975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.501646</td>\n",
       "      <td>0.554037</td>\n",
       "      <td>0.499443</td>\n",
       "      <td>0.492848</td>\n",
       "      <td>0.516364</td>\n",
       "      <td>0.513032</td>\n",
       "      <td>0.507250</td>\n",
       "      <td>0.445175</td>\n",
       "      <td>0.381707</td>\n",
       "      <td>0.315725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.649602</td>\n",
       "      <td>0.493519</td>\n",
       "      <td>0.429128</td>\n",
       "      <td>0.525793</td>\n",
       "      <td>0.467436</td>\n",
       "      <td>0.519316</td>\n",
       "      <td>0.382759</td>\n",
       "      <td>0.525866</td>\n",
       "      <td>0.557426</td>\n",
       "      <td>0.573915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.406618</td>\n",
       "      <td>0.570615</td>\n",
       "      <td>0.605580</td>\n",
       "      <td>0.630158</td>\n",
       "      <td>0.482372</td>\n",
       "      <td>0.645900</td>\n",
       "      <td>0.546970</td>\n",
       "      <td>0.496618</td>\n",
       "      <td>0.540319</td>\n",
       "      <td>0.330437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.569918</td>\n",
       "      <td>0.456317</td>\n",
       "      <td>0.277657</td>\n",
       "      <td>0.612645</td>\n",
       "      <td>0.455834</td>\n",
       "      <td>0.386266</td>\n",
       "      <td>0.635489</td>\n",
       "      <td>0.321914</td>\n",
       "      <td>0.469302</td>\n",
       "      <td>0.494628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500627</td>\n",
       "      <td>0.426700</td>\n",
       "      <td>0.416226</td>\n",
       "      <td>0.534267</td>\n",
       "      <td>0.498443</td>\n",
       "      <td>0.401804</td>\n",
       "      <td>0.521681</td>\n",
       "      <td>0.423616</td>\n",
       "      <td>0.518691</td>\n",
       "      <td>0.412501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.550918</td>\n",
       "      <td>0.558191</td>\n",
       "      <td>0.390285</td>\n",
       "      <td>0.534186</td>\n",
       "      <td>0.529445</td>\n",
       "      <td>0.337371</td>\n",
       "      <td>0.437665</td>\n",
       "      <td>0.427822</td>\n",
       "      <td>0.503292</td>\n",
       "      <td>0.497031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.570611</td>\n",
       "      <td>0.609020</td>\n",
       "      <td>0.520297</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.476385</td>\n",
       "      <td>0.623301</td>\n",
       "      <td>0.512758</td>\n",
       "      <td>0.416688</td>\n",
       "      <td>0.362978</td>\n",
       "      <td>0.429364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.571994  0.623757  0.309427  0.718099  0.373818  0.508229  0.503443   \n",
       "1  0.462480  0.471853  0.388536  0.531941  0.497407  0.382798  0.434003   \n",
       "2  0.649602  0.493519  0.429128  0.525793  0.467436  0.519316  0.382759   \n",
       "3  0.569918  0.456317  0.277657  0.612645  0.455834  0.386266  0.635489   \n",
       "4  0.550918  0.558191  0.390285  0.534186  0.529445  0.337371  0.437665   \n",
       "\n",
       "        7         8         9    ...       190       191       192       193  \\\n",
       "0  0.526289  0.426138  0.476286  ...  0.647968  0.610293  0.517064  0.644147   \n",
       "1  0.361507  0.443034  0.431975  ...  0.501646  0.554037  0.499443  0.492848   \n",
       "2  0.525866  0.557426  0.573915  ...  0.406618  0.570615  0.605580  0.630158   \n",
       "3  0.321914  0.469302  0.494628  ...  0.500627  0.426700  0.416226  0.534267   \n",
       "4  0.427822  0.503292  0.497031  ...  0.570611  0.609020  0.520297  0.527273   \n",
       "\n",
       "        194       195       196       197       198       199  \n",
       "0  0.564423  0.245045  0.510967  0.407351  0.508058  0.647381  \n",
       "1  0.516364  0.513032  0.507250  0.445175  0.381707  0.315725  \n",
       "2  0.482372  0.645900  0.546970  0.496618  0.540319  0.330437  \n",
       "3  0.498443  0.401804  0.521681  0.423616  0.518691  0.412501  \n",
       "4  0.476385  0.623301  0.512758  0.416688  0.362978  0.429364  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NORMALIZING\n",
    "\n",
    "wordvec_df = (wordvec_df - wordvec_df.min()) / (wordvec_df.max() - wordvec_df.min())\n",
    "wordvec_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f976ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39740, 200) (9935, 200) (39740,) (9935,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    wordvec_df.squeeze(), df['category'], test_size=.2, stratify=df['label'], random_state=128)\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fcbbad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(x_train.values).float()\n",
    "x_test = torch.tensor(x_test.values).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a6d197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(y_train.values)\n",
    "y_test = torch.tensor(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35671cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_encoding(nd_array):\n",
    "    predictions = nd_array\n",
    "    \n",
    "    ps = torch.exp(predictions)\n",
    "    top_p, top_class  = ps.topk(1, dim=1)\n",
    "    \n",
    "\n",
    "    return top_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1222a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.hidden_layer_1 = nn.Linear(x_train.shape[1], 64) # input to first hidden layer\n",
    "        self.output_layer = nn.Linear(64, self.out_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hidden_layer_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        y = self.output_layer(x)\n",
    "        y = self.activation(y)\n",
    "        y = self.softmax(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33d4a429",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_neural = NeuralNetwork(x_train.shape[1], df['category'].nunique())\n",
    "# model = NeuralNetwork(x_train.shape[1], 5)\n",
    "\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.Adam(model_neural.parameters(), lr=0.002)\n",
    "\n",
    "#setting up scheduler\n",
    "# scheduler = ReduceLROnPlateau(optimizer, 'min', patience = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa76472d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.471\n",
      "Epoch: 2/100..  Training Loss: 1.004..  Test Loss: 1.008..  Test Accuracy: 0.471\n",
      "Epoch: 3/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.470\n",
      "Epoch: 4/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 5/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.470\n",
      "Epoch: 6/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 7/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.470\n",
      "Epoch: 8/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.470\n",
      "Epoch: 9/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 10/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 11/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 12/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 13/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 14/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.470\n",
      "Epoch: 15/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 16/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 17/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 18/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.468\n",
      "Epoch: 19/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.470\n",
      "Epoch: 20/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.470\n",
      "Epoch: 21/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.468\n",
      "Epoch: 22/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 23/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.470\n",
      "Epoch: 24/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.468\n",
      "Epoch: 25/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 26/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.470\n",
      "Epoch: 27/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.470\n",
      "Epoch: 28/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.470\n",
      "Epoch: 29/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.468\n",
      "Epoch: 30/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 31/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 32/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.468\n",
      "Epoch: 33/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.470\n",
      "Epoch: 34/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.467\n",
      "Epoch: 35/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 36/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 37/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.467\n",
      "Epoch: 38/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 39/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 40/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.467\n",
      "Epoch: 41/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 42/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.470\n",
      "Epoch: 43/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.467\n",
      "Epoch: 44/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.470\n",
      "Epoch: 45/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.468\n",
      "Epoch: 46/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.468\n",
      "Epoch: 47/100..  Training Loss: 1.003..  Test Loss: 1.008..  Test Accuracy: 0.468\n",
      "Epoch: 48/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.468\n",
      "Epoch: 49/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.468\n",
      "Epoch: 50/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.470\n",
      "Epoch: 51/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.467\n",
      "Epoch: 52/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 53/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.468\n",
      "Epoch: 54/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.468\n",
      "Epoch: 55/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 56/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.468\n",
      "Epoch: 57/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.468\n",
      "Epoch: 58/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.467\n",
      "Epoch: 59/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.468\n",
      "Epoch: 60/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.470\n",
      "Epoch: 61/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.467\n",
      "Epoch: 62/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 63/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 64/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.468\n",
      "Epoch: 65/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 66/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.466\n",
      "Epoch: 67/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 68/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.467\n",
      "Epoch: 69/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.467\n",
      "Epoch: 70/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.470\n",
      "Epoch: 71/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.467\n",
      "Epoch: 72/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 73/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 74/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.468\n",
      "Epoch: 75/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.466\n",
      "Epoch: 76/100..  Training Loss: 1.001..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 77/100..  Training Loss: 1.001..  Test Loss: 1.008..  Test Accuracy: 0.467\n",
      "Epoch: 78/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 79/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.468\n",
      "Epoch: 80/100..  Training Loss: 1.001..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 81/100..  Training Loss: 1.001..  Test Loss: 1.008..  Test Accuracy: 0.468\n",
      "Epoch: 82/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.467\n",
      "Epoch: 83/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 84/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 85/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.467\n",
      "Epoch: 86/100..  Training Loss: 1.001..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 87/100..  Training Loss: 1.001..  Test Loss: 1.008..  Test Accuracy: 0.465\n",
      "Epoch: 88/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.468\n",
      "Epoch: 89/100..  Training Loss: 1.001..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 90/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.466\n",
      "Epoch: 91/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.468\n",
      "Epoch: 92/100..  Training Loss: 1.001..  Test Loss: 1.008..  Test Accuracy: 0.466\n",
      "Epoch: 93/100..  Training Loss: 1.001..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 94/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 95/100..  Training Loss: 1.001..  Test Loss: 1.008..  Test Accuracy: 0.467\n",
      "Epoch: 96/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 97/100..  Training Loss: 1.002..  Test Loss: 1.008..  Test Accuracy: 0.466\n",
      "Epoch: 98/100..  Training Loss: 1.001..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 99/100..  Training Loss: 1.001..  Test Loss: 1.008..  Test Accuracy: 0.469\n",
      "Epoch: 100/100..  Training Loss: 1.001..  Test Loss: 1.008..  Test Accuracy: 0.466\n",
      "CPU times: total: 21.1 s\n",
      "Wall time: 5.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "epochs = 100\n",
    "for e in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model_neural.forward(x_train)\n",
    "    loss = criterion(output, y_train)\n",
    "    loss.backward()\n",
    "    train_loss = loss.item()\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    # Turn off gradients for validation, saves memory and computations\n",
    "    with torch.no_grad():\n",
    "        model_neural.eval()\n",
    "        log_ps = model_neural.forward(x_test)\n",
    "        test_loss = criterion(log_ps, y_test)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        ps = torch.exp(log_ps)\n",
    "        top_p, top_class  = ps.topk(1, dim=1)\n",
    "        equals = top_class == y_test.view(*top_class.shape)\n",
    "        test_accuracy = torch.mean(equals.float())\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "    model_neural.train()\n",
    "\n",
    "    print(f\"Epoch: {e+1}/{epochs}.. \",\n",
    "          f\"Training Loss: {train_loss:.3f}.. \",\n",
    "          f\"Test Loss: {test_loss:.3f}.. \",\n",
    "          f\"Test Accuracy: {test_accuracy:.3f}\")\n",
    "#     scheduler.step(test_loss/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "518ed4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "488818ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        ...,\n",
       "        [2],\n",
       "        [2],\n",
       "        [1]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model_neural(x_test)\n",
    "preds = topk_encoding(preds)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9920fa4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4638147961751384"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_score = accuracy_score(y_test, preds)\n",
    "\n",
    "acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "766e644d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32795210974961847"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score = f1_score(y_test, preds, average='macro')\n",
    "f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17617894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae878f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Custom dataset to treat how the model picks an x, y combination from the dataset\n",
    "# class MyCustomDataset(Dataset):\n",
    "#     def __init__(self, x, y):\n",
    "#         self.x = x\n",
    "#         self.y = y\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.x)\n",
    "    \n",
    "#     # Requires you to return data as a pair of _x, _y\n",
    "#     def __getitem__(self, index):\n",
    "#         return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff3efcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_dataset_train = MyCustomDataset(x=x_train, y=y_train)\n",
    "# custom_dataset_test = MyCustomDataset(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "575f611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = DataLoader(\n",
    "#     custom_dataset_train,\n",
    "#     batch_size=16,\n",
    "#     shuffle=False,\n",
    "#     drop_last=False\n",
    "# )\n",
    "\n",
    "# test_loader = DataLoader(\n",
    "#     custom_dataset_test,\n",
    "#     batch_size=16,\n",
    "#     shuffle=False,\n",
    "#     drop_last=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3fc4ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# train_losses = []\n",
    "# test_losses = []\n",
    "# test_accuracies = []\n",
    "\n",
    "# epochs = 50\n",
    "# for e in range(epochs):\n",
    "    \n",
    "#     for data, label in train_loader:\n",
    "    \n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         output = model_neural.forward(data)\n",
    "#         loss = criterion(output, label)\n",
    "#         loss.backward()\n",
    "#         train_loss = loss.item()\n",
    "#         train_losses.append(train_loss)\n",
    "    \n",
    "#         optimizer.step()\n",
    "\n",
    "#     # Turn off gradients for validation, saves memory and computations\n",
    "#     with torch.no_grad():\n",
    "#         model_neural.eval()\n",
    "#         for data, label in test_loader:\n",
    "#             log_ps = model_neural.forward(data)\n",
    "#             test_loss = criterion(log_ps, label)\n",
    "#             test_losses.append(test_loss)\n",
    "\n",
    "#             ps = torch.exp(log_ps)\n",
    "#             top_p, top_class  = ps.topk(1, dim=1)\n",
    "#             equals = top_class == label.view(*top_class.shape)\n",
    "#             test_accuracy = torch.mean(equals.float())\n",
    "#             test_accuracies.append(test_accuracy)\n",
    "#         model_neural.train()\n",
    "\n",
    "#         print(f\"Epoch: {e+1}/{epochs}.. \",\n",
    "#               f\"Training Loss: {train_loss:.3f}.. \",\n",
    "#               f\"Test Loss: {test_loss:.3f}.. \",\n",
    "#               f\"Test Accuracy: {test_accuracy:.3f}\")\n",
    "#         scheduler.step(test_loss/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b80a3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c432bef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
