{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26932adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import joblib\n",
    "from joblib import load\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2baa8e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4bab239",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('twitter_class.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b638d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1).reset_index() #shuffling data in pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95643faf",
   "metadata": {},
   "source": [
    "### Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03f0af9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(message):\n",
    "    result = message.lower()\n",
    "    return result\n",
    "\n",
    "def remove_num(message):\n",
    "    result = re.sub(r'\\d+','',message)\n",
    "    return result\n",
    "\n",
    "def contractions(message):\n",
    "     result = re.sub(r\"won't\", \"will not\",message)\n",
    "     result = re.sub(r\"would't\", \"would not\",message)\n",
    "     result = re.sub(r\"could't\", \"could not\",message)\n",
    "     result = re.sub(r\"\\'d\", \" would\",message)\n",
    "     result = re.sub(r\"can\\'t\", \"can not\",message)\n",
    "     result = re.sub(r\"n\\'t\", \" not\", message)\n",
    "     result = re.sub(r\"\\'re\", \" are\", message)\n",
    "     result = re.sub(r\"\\'s\", \" is\", message)\n",
    "     result = re.sub(r\"\\'ll\", \" will\", message)\n",
    "     result = re.sub(r\"\\'t\", \" not\", message)\n",
    "     result = re.sub(r\"\\'ve\", \" have\", message)\n",
    "     result = re.sub(r\"\\'m\", \" am\", message)\n",
    "     return result\n",
    "    \n",
    "def remove_punctuation(message):\n",
    "    result = message.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
    "    return result\n",
    "\n",
    "def remove_whitespace(message):\n",
    "    result = message.strip()\n",
    "    result = re.sub(' +',' ',message)\n",
    "    return result\n",
    "\n",
    "def replace_newline(message):\n",
    "    result = message.replace('\\n','')\n",
    "    return result\n",
    "\n",
    "def data_cleanup(message):\n",
    "    cleaning_utils = [to_lower, remove_num, contractions, remove_punctuation, remove_whitespace, replace_newline]\n",
    "    for util in cleaning_utils:\n",
    "        message = util(message)\n",
    "    return message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aeae90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12004</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.002682</td>\n",
       "      <td>0.029541</td>\n",
       "      <td>0.002964</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>0.001822</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.000773</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.002686</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000206</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40755</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1681</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.000326</td>\n",
       "      <td>0.014041</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.002483</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.001436</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.001759</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51023</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index         0         1         2         3         4         5  \\\n",
       "0  29085  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  12004  0.001063  0.000697  0.002682  0.029541  0.002964  0.000161   \n",
       "2  40755  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3   1681  0.000381  0.000198  0.000326  0.014041  0.000882  0.000095   \n",
       "4  51023  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "          6         7         8  ...      1991      1992      1993      1994  \\\n",
       "0  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.001822  0.000491  0.000029  ...  0.000375  0.000773  0.000777  0.001049   \n",
       "2  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.002483  0.000146  0.000004  ...  0.000049  0.000235  0.000113  0.001436   \n",
       "4  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "       1995      1996      1997      1998      1999  category  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000         1  \n",
       "1  0.000132  0.002686  0.000204  0.000206  0.000176         0  \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000         2  \n",
       "3  0.000552  0.001759  0.000207  0.000148  0.000037         0  \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000         2  \n",
       "\n",
       "[5 rows x 2002 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a1b1562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df.iloc[:,1:-1], df['category'], test_size=.2, stratify=df['category'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4298ee74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "0    3000\n",
       "1    4451\n",
       "2    3935\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.DataFrame(y_test)\n",
    "a.groupby(['category']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4582a3",
   "metadata": {},
   "source": [
    "### Building  Neural Network Model\n",
    "\n",
    "--no need to feed the x_train and x_test to tfidf vectorizer since it is already converted in separate notebook. Refer to NN_model with AutoEncoder.ipynb\n",
    "\n",
    "\n",
    "--proceed to convert data to tensors\n",
    "\n",
    "##### Train Datasets:\n",
    "\n",
    "0   ->  12000\n",
    "\n",
    "1   ->  17804\n",
    "\n",
    "2   ->  15737\n",
    "\n",
    "\n",
    "##### Test Datasets\n",
    "\n",
    "0   ->  3000\n",
    "\n",
    "1   ->  4451\n",
    "\n",
    "2   ->  3935"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "349e550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(x_train.values).float()\n",
    "x_test = torch.tensor(x_test.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ad06c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(y_train.values)\n",
    "y_test = torch.tensor(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54866aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_encoding(nd_array):\n",
    "    \"\"\"\n",
    "    Function to flatten the predicted category\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = nd_array\n",
    "    \n",
    "    ps = torch.exp(predictions)\n",
    "    top_p, top_class  = ps.topk(1, dim=1)\n",
    "    \n",
    "\n",
    "    return top_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfa9fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.hidden_layer_1 = nn.Linear(x_train.shape[1], 64) # input to first hidden layer\n",
    "        self.output_layer = nn.Linear(64, self.out_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hidden_layer_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        y = self.output_layer(x)\n",
    "        y = self.activation(y)\n",
    "        y = self.softmax(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "371e2c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(x_train.shape[1], df['category'].nunique())\n",
    "# model = NeuralNetwork(x_train.shape[1], 5)\n",
    "\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "#setting up scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da36d89",
   "metadata": {},
   "source": [
    "### Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6f106f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100..  Training Loss: 0.350..  Test Loss: 0.474..  Test Accuracy: 0.789\n",
      "Epoch: 2/100..  Training Loss: 0.349..  Test Loss: 0.473..  Test Accuracy: 0.789\n",
      "Epoch: 3/100..  Training Loss: 0.347..  Test Loss: 0.473..  Test Accuracy: 0.789\n",
      "Epoch: 4/100..  Training Loss: 0.347..  Test Loss: 0.473..  Test Accuracy: 0.789\n",
      "Epoch: 5/100..  Training Loss: 0.346..  Test Loss: 0.473..  Test Accuracy: 0.789\n",
      "Epoch: 6/100..  Training Loss: 0.346..  Test Loss: 0.472..  Test Accuracy: 0.788\n",
      "Epoch: 7/100..  Training Loss: 0.345..  Test Loss: 0.472..  Test Accuracy: 0.789\n",
      "Epoch: 8/100..  Training Loss: 0.343..  Test Loss: 0.472..  Test Accuracy: 0.789\n",
      "Epoch: 9/100..  Training Loss: 0.344..  Test Loss: 0.472..  Test Accuracy: 0.789\n",
      "Epoch: 10/100..  Training Loss: 0.343..  Test Loss: 0.471..  Test Accuracy: 0.789\n",
      "Epoch: 11/100..  Training Loss: 0.342..  Test Loss: 0.471..  Test Accuracy: 0.789\n",
      "Epoch: 12/100..  Training Loss: 0.341..  Test Loss: 0.471..  Test Accuracy: 0.789\n",
      "Epoch: 13/100..  Training Loss: 0.341..  Test Loss: 0.471..  Test Accuracy: 0.789\n",
      "Epoch: 14/100..  Training Loss: 0.340..  Test Loss: 0.471..  Test Accuracy: 0.789\n",
      "Epoch: 15/100..  Training Loss: 0.339..  Test Loss: 0.470..  Test Accuracy: 0.789\n",
      "Epoch: 16/100..  Training Loss: 0.337..  Test Loss: 0.470..  Test Accuracy: 0.789\n",
      "Epoch: 17/100..  Training Loss: 0.337..  Test Loss: 0.470..  Test Accuracy: 0.789\n",
      "Epoch: 18/100..  Training Loss: 0.336..  Test Loss: 0.470..  Test Accuracy: 0.789\n",
      "Epoch: 19/100..  Training Loss: 0.336..  Test Loss: 0.470..  Test Accuracy: 0.790\n",
      "Epoch: 20/100..  Training Loss: 0.335..  Test Loss: 0.470..  Test Accuracy: 0.790\n",
      "Epoch: 21/100..  Training Loss: 0.334..  Test Loss: 0.469..  Test Accuracy: 0.789\n",
      "Epoch: 22/100..  Training Loss: 0.334..  Test Loss: 0.469..  Test Accuracy: 0.789\n",
      "Epoch: 23/100..  Training Loss: 0.333..  Test Loss: 0.469..  Test Accuracy: 0.790\n",
      "Epoch: 24/100..  Training Loss: 0.333..  Test Loss: 0.469..  Test Accuracy: 0.790\n",
      "Epoch: 25/100..  Training Loss: 0.331..  Test Loss: 0.469..  Test Accuracy: 0.790\n",
      "Epoch: 26/100..  Training Loss: 0.330..  Test Loss: 0.468..  Test Accuracy: 0.791\n",
      "Epoch: 27/100..  Training Loss: 0.331..  Test Loss: 0.468..  Test Accuracy: 0.791\n",
      "Epoch: 28/100..  Training Loss: 0.329..  Test Loss: 0.468..  Test Accuracy: 0.791\n",
      "Epoch: 29/100..  Training Loss: 0.330..  Test Loss: 0.468..  Test Accuracy: 0.791\n",
      "Epoch: 30/100..  Training Loss: 0.328..  Test Loss: 0.468..  Test Accuracy: 0.790\n",
      "Epoch: 31/100..  Training Loss: 0.327..  Test Loss: 0.468..  Test Accuracy: 0.790\n",
      "Epoch: 32/100..  Training Loss: 0.327..  Test Loss: 0.468..  Test Accuracy: 0.790\n",
      "Epoch: 33/100..  Training Loss: 0.326..  Test Loss: 0.468..  Test Accuracy: 0.790\n",
      "Epoch: 34/100..  Training Loss: 0.325..  Test Loss: 0.467..  Test Accuracy: 0.791\n",
      "Epoch: 35/100..  Training Loss: 0.324..  Test Loss: 0.467..  Test Accuracy: 0.791\n",
      "Epoch: 36/100..  Training Loss: 0.324..  Test Loss: 0.467..  Test Accuracy: 0.791\n",
      "Epoch: 37/100..  Training Loss: 0.323..  Test Loss: 0.467..  Test Accuracy: 0.791\n",
      "Epoch: 38/100..  Training Loss: 0.320..  Test Loss: 0.467..  Test Accuracy: 0.791\n",
      "Epoch: 39/100..  Training Loss: 0.322..  Test Loss: 0.467..  Test Accuracy: 0.791\n",
      "Epoch: 40/100..  Training Loss: 0.320..  Test Loss: 0.467..  Test Accuracy: 0.791\n",
      "Epoch: 41/100..  Training Loss: 0.320..  Test Loss: 0.467..  Test Accuracy: 0.791\n",
      "Epoch: 42/100..  Training Loss: 0.319..  Test Loss: 0.467..  Test Accuracy: 0.791\n",
      "Epoch: 43/100..  Training Loss: 0.319..  Test Loss: 0.467..  Test Accuracy: 0.791\n",
      "Epoch: 44/100..  Training Loss: 0.318..  Test Loss: 0.467..  Test Accuracy: 0.791\n",
      "Epoch: 45/100..  Training Loss: 0.317..  Test Loss: 0.467..  Test Accuracy: 0.791\n",
      "Epoch: 46/100..  Training Loss: 0.316..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 47/100..  Training Loss: 0.316..  Test Loss: 0.466..  Test Accuracy: 0.791\n",
      "Epoch: 48/100..  Training Loss: 0.316..  Test Loss: 0.466..  Test Accuracy: 0.791\n",
      "Epoch: 49/100..  Training Loss: 0.314..  Test Loss: 0.466..  Test Accuracy: 0.791\n",
      "Epoch: 50/100..  Training Loss: 0.315..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 51/100..  Training Loss: 0.313..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 52/100..  Training Loss: 0.313..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 53/100..  Training Loss: 0.311..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 54/100..  Training Loss: 0.313..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 55/100..  Training Loss: 0.311..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 56/100..  Training Loss: 0.310..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 57/100..  Training Loss: 0.309..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 58/100..  Training Loss: 0.309..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 59/100..  Training Loss: 0.308..  Test Loss: 0.466..  Test Accuracy: 0.791\n",
      "Epoch: 60/100..  Training Loss: 0.307..  Test Loss: 0.466..  Test Accuracy: 0.791\n",
      "Epoch: 61/100..  Training Loss: 0.306..  Test Loss: 0.466..  Test Accuracy: 0.791\n",
      "Epoch: 62/100..  Training Loss: 0.305..  Test Loss: 0.466..  Test Accuracy: 0.791\n",
      "Epoch: 63/100..  Training Loss: 0.305..  Test Loss: 0.466..  Test Accuracy: 0.791\n",
      "Epoch: 64/100..  Training Loss: 0.304..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 65/100..  Training Loss: 0.304..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 66/100..  Training Loss: 0.303..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 67/100..  Training Loss: 0.303..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 68/100..  Training Loss: 0.301..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 69/100..  Training Loss: 0.300..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 70/100..  Training Loss: 0.300..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 71/100..  Training Loss: 0.301..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 72/100..  Training Loss: 0.299..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 73/100..  Training Loss: 0.297..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 74/100..  Training Loss: 0.299..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 75/100..  Training Loss: 0.298..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 76/100..  Training Loss: 0.296..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 77/100..  Training Loss: 0.296..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 78/100..  Training Loss: 0.295..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 79/100..  Training Loss: 0.294..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 80/100..  Training Loss: 0.294..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 81/100..  Training Loss: 0.293..  Test Loss: 0.466..  Test Accuracy: 0.793\n",
      "Epoch: 82/100..  Training Loss: 0.293..  Test Loss: 0.466..  Test Accuracy: 0.793\n",
      "Epoch: 83/100..  Training Loss: 0.293..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 84/100..  Training Loss: 0.290..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 85/100..  Training Loss: 0.290..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 86/100..  Training Loss: 0.291..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 87/100..  Training Loss: 0.289..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 88/100..  Training Loss: 0.288..  Test Loss: 0.466..  Test Accuracy: 0.793\n",
      "Epoch: 89/100..  Training Loss: 0.287..  Test Loss: 0.466..  Test Accuracy: 0.793\n",
      "Epoch: 90/100..  Training Loss: 0.287..  Test Loss: 0.466..  Test Accuracy: 0.793\n",
      "Epoch: 91/100..  Training Loss: 0.288..  Test Loss: 0.466..  Test Accuracy: 0.793\n",
      "Epoch: 92/100..  Training Loss: 0.288..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 93/100..  Training Loss: 0.287..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 94/100..  Training Loss: 0.285..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 95/100..  Training Loss: 0.288..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 96/100..  Training Loss: 0.288..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 97/100..  Training Loss: 0.287..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 98/100..  Training Loss: 0.286..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 99/100..  Training Loss: 0.286..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "Epoch: 100/100..  Training Loss: 0.287..  Test Loss: 0.466..  Test Accuracy: 0.792\n",
      "CPU times: total: 2min 4s\n",
      "Wall time: 38.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "epochs = 400\n",
    "for e in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model.forward(x_train) #Forward pass, get the logits\n",
    "    loss = criterion(output, y_train) # Calculate the loss with the logits and the labels\n",
    "    loss.backward()\n",
    "    train_loss = loss.item()\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    # Turn off gradients for validation, saves memory and computations\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        log_ps = model.forward(x_test)\n",
    "        test_loss = criterion(log_ps, y_test)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        ps = torch.exp(log_ps)\n",
    "        top_p, top_class  = ps.topk(1, dim=1)\n",
    "        equals = top_class == y_test.view(*top_class.shape)\n",
    "        test_accuracy = torch.mean(equals.float())\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    print(f\"Epoch: {e+1}/{epochs}.. \",\n",
    "          f\"Training Loss: {train_loss:.3f}.. \",\n",
    "          f\"Test Loss: {test_loss:.3f}.. \",\n",
    "          f\"Test Accuracy: {test_accuracy:.3f}\")\n",
    "    scheduler.step(test_loss/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20e22840",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"nn_wauto.joblib\", \"wb+\") as filename:\n",
    "        joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e88e68cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.forward(x_test)\n",
    "preds = topk_encoding(preds)\n",
    "preds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8eeb002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7885122079747058"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_score = accuracy_score(y_test, preds)\n",
    "acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "90b29486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8018723520537367"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score = f1_score(y_test, preds, average='macro')\n",
    "f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1269852a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Classification Report --\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96      3000\n",
      "           1       0.73      0.75      0.74      4451\n",
      "           2       0.73      0.67      0.70      3935\n",
      "\n",
      "    accuracy                           0.79     11386\n",
      "   macro avg       0.80      0.81      0.80     11386\n",
      "weighted avg       0.78      0.79      0.79     11386\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report_str = classification_report(y_test, preds)\n",
    "print(\"-- Classification Report --\")\n",
    "print(report_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1257f103",
   "metadata": {},
   "source": [
    "### Simple ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "69fe8174",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = load(\"tfidf_vectorizer.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc4ef20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_vectorizer(message):\n",
    "    \"\"\"\n",
    "    Function to predict the category of inputted message\n",
    "    \"\"\"\n",
    "    \n",
    "    cleaned_message = pd.Series(message).apply(lambda x: data_cleanup(x))\n",
    "    vec = vectorizer.transform(pd.Series(cleaned_message))\n",
    "    vec = torch.tensor(scipy.sparse.csr_matrix.todense(vec)).float()\n",
    "    preds = model.forward(vec)\n",
    "    category = topk_encoding(preds).detach().cpu().numpy()\n",
    "    \n",
    "    return int(category[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "446869f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = input_vectorizer(\"this product is trash\")\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aeb1a287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = input_vectorizer(\"prenatal move to wednesday at 8 pm starting tonight..\")\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c8d4c798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = input_vectorizer(\"nice video\")\n",
    "b\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
