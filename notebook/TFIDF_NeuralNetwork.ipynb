{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26932adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2baa8e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4bab239",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('twitter_cleaned.csv')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6aeae90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>prenatal move to wednesday at pm starting toni...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>happy nd birthday to prince george i cant beli...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>do not be afraid to be saint be open to the lo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>dst is saturday nightsunday morning got ta be ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>sony reward app is like a lot of yo female sin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                            message  category\n",
       "0   neutral  prenatal move to wednesday at pm starting toni...         1\n",
       "1  positive  happy nd birthday to prince george i cant beli...         2\n",
       "2  positive  do not be afraid to be saint be open to the lo...         2\n",
       "3   neutral  dst is saturday nightsunday morning got ta be ...         1\n",
       "4  negative  sony reward app is like a lot of yo female sin...         0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a1b1562a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39740,) (9935,) (39740,) (9935,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df['message'], df['category'], test_size=.2, stratify=df['label'], random_state=0)\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "516af63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=2000)\n",
    "# vectorizer = TfidfVectorizer(max_features=1500, stop_words='english')\n",
    "\n",
    "# Learn vocabulary from training texts and vectorize training texts.\n",
    "x_train = vectorizer.fit_transform(x_train)\n",
    "\n",
    "# Vectorize test texts.\n",
    "x_test = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "349e550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(scipy.sparse.csr_matrix.todense(x_train)).float()\n",
    "x_test = torch.tensor(scipy.sparse.csr_matrix.todense(x_test)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4ad06c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(y_train.values)\n",
    "y_test = torch.tensor(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54866aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_encoding(nd_array):\n",
    "    \"\"\"\n",
    "    Function to flatten the predicted category\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = nd_array\n",
    "    \n",
    "    ps = torch.exp(predictions)\n",
    "    top_p, top_class  = ps.topk(1, dim=1)\n",
    "    \n",
    "\n",
    "    return top_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfa9fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.hidden_layer_1 = nn.Linear(x_train.shape[1], 64) # input to first hidden layer\n",
    "        self.output_layer = nn.Linear(64, self.out_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hidden_layer_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        y = self.output_layer(x)\n",
    "        y = self.activation(y)\n",
    "        y = self.softmax(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "371e2c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(x_train.shape[1], df['category'].nunique())\n",
    "# model = NeuralNetwork(x_train.shape[1], 5)\n",
    "\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "#setting up scheduler\n",
    "# scheduler = ReduceLROnPlateau(optimizer, 'min', patience = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f1973eaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "nll_loss_nd(): argument 'target' (position 2) must be Tensor, not Series",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m logps \u001b[38;5;241m=\u001b[39m model(x_train)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Calculate the loss with the logits and the labels\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Optimizers require the parameters to optimize and a learning rate\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\pychat\\chatbot\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Desktop\\pychat\\chatbot\\lib\\site-packages\\torch\\nn\\modules\\loss.py:216\u001b[0m, in \u001b[0;36mNLLLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\pychat\\chatbot\\lib\\site-packages\\torch\\nn\\functional.py:2701\u001b[0m, in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2700\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 2701\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnll_loss_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: nll_loss_nd(): argument 'target' (position 2) must be Tensor, not Series"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(x_train.shape[1], 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(0.1),\n",
    "                      nn.Linear(64, df['category'].nunique()),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Forward pass, get our logits\n",
    "logps = model(x_train)\n",
    "# Calculate the loss with the logits and the labels\n",
    "loss = criterion(logps, y_train)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6f106f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200..  Training Loss: 1.132..  Test Loss: 1.125..  Test Accuracy: 0.156\n",
      "Epoch: 2/200..  Training Loss: 1.125..  Test Loss: 1.119..  Test Accuracy: 0.156\n",
      "Epoch: 3/200..  Training Loss: 1.119..  Test Loss: 1.112..  Test Accuracy: 0.156\n",
      "Epoch: 4/200..  Training Loss: 1.112..  Test Loss: 1.105..  Test Accuracy: 0.156\n",
      "Epoch: 5/200..  Training Loss: 1.105..  Test Loss: 1.098..  Test Accuracy: 0.264\n",
      "Epoch: 6/200..  Training Loss: 1.097..  Test Loss: 1.090..  Test Accuracy: 0.492\n",
      "Epoch: 7/200..  Training Loss: 1.090..  Test Loss: 1.083..  Test Accuracy: 0.537\n",
      "Epoch: 8/200..  Training Loss: 1.082..  Test Loss: 1.075..  Test Accuracy: 0.536\n",
      "Epoch: 9/200..  Training Loss: 1.074..  Test Loss: 1.067..  Test Accuracy: 0.534\n",
      "Epoch: 10/200..  Training Loss: 1.065..  Test Loss: 1.059..  Test Accuracy: 0.533\n",
      "Epoch: 11/200..  Training Loss: 1.057..  Test Loss: 1.051..  Test Accuracy: 0.533\n",
      "Epoch: 12/200..  Training Loss: 1.049..  Test Loss: 1.043..  Test Accuracy: 0.535\n",
      "Epoch: 13/200..  Training Loss: 1.041..  Test Loss: 1.036..  Test Accuracy: 0.537\n",
      "Epoch: 14/200..  Training Loss: 1.033..  Test Loss: 1.028..  Test Accuracy: 0.539\n",
      "Epoch: 15/200..  Training Loss: 1.025..  Test Loss: 1.021..  Test Accuracy: 0.540\n",
      "Epoch: 16/200..  Training Loss: 1.017..  Test Loss: 1.014..  Test Accuracy: 0.543\n",
      "Epoch: 17/200..  Training Loss: 1.010..  Test Loss: 1.007..  Test Accuracy: 0.547\n",
      "Epoch: 18/200..  Training Loss: 1.003..  Test Loss: 1.001..  Test Accuracy: 0.549\n",
      "Epoch: 19/200..  Training Loss: 0.996..  Test Loss: 0.995..  Test Accuracy: 0.551\n",
      "Epoch: 20/200..  Training Loss: 0.989..  Test Loss: 0.989..  Test Accuracy: 0.555\n",
      "Epoch: 21/200..  Training Loss: 0.983..  Test Loss: 0.984..  Test Accuracy: 0.557\n",
      "Epoch: 22/200..  Training Loss: 0.978..  Test Loss: 0.979..  Test Accuracy: 0.558\n",
      "Epoch: 23/200..  Training Loss: 0.972..  Test Loss: 0.974..  Test Accuracy: 0.562\n",
      "Epoch: 24/200..  Training Loss: 0.967..  Test Loss: 0.970..  Test Accuracy: 0.566\n",
      "Epoch: 25/200..  Training Loss: 0.962..  Test Loss: 0.966..  Test Accuracy: 0.568\n",
      "Epoch: 26/200..  Training Loss: 0.957..  Test Loss: 0.961..  Test Accuracy: 0.571\n",
      "Epoch: 27/200..  Training Loss: 0.952..  Test Loss: 0.958..  Test Accuracy: 0.573\n",
      "Epoch: 28/200..  Training Loss: 0.947..  Test Loss: 0.954..  Test Accuracy: 0.575\n",
      "Epoch: 29/200..  Training Loss: 0.943..  Test Loss: 0.950..  Test Accuracy: 0.577\n",
      "Epoch: 30/200..  Training Loss: 0.938..  Test Loss: 0.946..  Test Accuracy: 0.579\n",
      "Epoch: 31/200..  Training Loss: 0.933..  Test Loss: 0.942..  Test Accuracy: 0.579\n",
      "Epoch: 32/200..  Training Loss: 0.928..  Test Loss: 0.938..  Test Accuracy: 0.581\n",
      "Epoch: 33/200..  Training Loss: 0.923..  Test Loss: 0.933..  Test Accuracy: 0.581\n",
      "Epoch: 34/200..  Training Loss: 0.918..  Test Loss: 0.929..  Test Accuracy: 0.583\n",
      "Epoch: 35/200..  Training Loss: 0.913..  Test Loss: 0.925..  Test Accuracy: 0.582\n",
      "Epoch: 36/200..  Training Loss: 0.908..  Test Loss: 0.920..  Test Accuracy: 0.582\n",
      "Epoch: 37/200..  Training Loss: 0.902..  Test Loss: 0.916..  Test Accuracy: 0.583\n",
      "Epoch: 38/200..  Training Loss: 0.897..  Test Loss: 0.912..  Test Accuracy: 0.584\n",
      "Epoch: 39/200..  Training Loss: 0.892..  Test Loss: 0.907..  Test Accuracy: 0.584\n",
      "Epoch: 40/200..  Training Loss: 0.886..  Test Loss: 0.903..  Test Accuracy: 0.584\n",
      "Epoch: 41/200..  Training Loss: 0.881..  Test Loss: 0.899..  Test Accuracy: 0.584\n",
      "Epoch: 42/200..  Training Loss: 0.876..  Test Loss: 0.895..  Test Accuracy: 0.585\n",
      "Epoch: 43/200..  Training Loss: 0.870..  Test Loss: 0.891..  Test Accuracy: 0.585\n",
      "Epoch: 44/200..  Training Loss: 0.866..  Test Loss: 0.887..  Test Accuracy: 0.587\n",
      "Epoch: 45/200..  Training Loss: 0.860..  Test Loss: 0.883..  Test Accuracy: 0.587\n",
      "Epoch: 46/200..  Training Loss: 0.856..  Test Loss: 0.880..  Test Accuracy: 0.588\n",
      "Epoch: 47/200..  Training Loss: 0.850..  Test Loss: 0.876..  Test Accuracy: 0.589\n",
      "Epoch: 48/200..  Training Loss: 0.846..  Test Loss: 0.873..  Test Accuracy: 0.590\n",
      "Epoch: 49/200..  Training Loss: 0.842..  Test Loss: 0.869..  Test Accuracy: 0.592\n",
      "Epoch: 50/200..  Training Loss: 0.837..  Test Loss: 0.866..  Test Accuracy: 0.593\n",
      "Epoch: 51/200..  Training Loss: 0.833..  Test Loss: 0.863..  Test Accuracy: 0.596\n",
      "Epoch: 52/200..  Training Loss: 0.828..  Test Loss: 0.860..  Test Accuracy: 0.596\n",
      "Epoch: 53/200..  Training Loss: 0.824..  Test Loss: 0.857..  Test Accuracy: 0.597\n",
      "Epoch: 54/200..  Training Loss: 0.821..  Test Loss: 0.854..  Test Accuracy: 0.598\n",
      "Epoch: 55/200..  Training Loss: 0.816..  Test Loss: 0.852..  Test Accuracy: 0.600\n",
      "Epoch: 56/200..  Training Loss: 0.812..  Test Loss: 0.849..  Test Accuracy: 0.601\n",
      "Epoch: 57/200..  Training Loss: 0.809..  Test Loss: 0.847..  Test Accuracy: 0.602\n",
      "Epoch: 58/200..  Training Loss: 0.805..  Test Loss: 0.844..  Test Accuracy: 0.604\n",
      "Epoch: 59/200..  Training Loss: 0.801..  Test Loss: 0.842..  Test Accuracy: 0.603\n",
      "Epoch: 60/200..  Training Loss: 0.798..  Test Loss: 0.840..  Test Accuracy: 0.605\n",
      "Epoch: 61/200..  Training Loss: 0.794..  Test Loss: 0.837..  Test Accuracy: 0.605\n",
      "Epoch: 62/200..  Training Loss: 0.791..  Test Loss: 0.835..  Test Accuracy: 0.606\n",
      "Epoch: 63/200..  Training Loss: 0.787..  Test Loss: 0.834..  Test Accuracy: 0.607\n",
      "Epoch: 64/200..  Training Loss: 0.784..  Test Loss: 0.832..  Test Accuracy: 0.608\n",
      "Epoch: 65/200..  Training Loss: 0.782..  Test Loss: 0.830..  Test Accuracy: 0.608\n",
      "Epoch: 66/200..  Training Loss: 0.778..  Test Loss: 0.828..  Test Accuracy: 0.609\n",
      "Epoch: 67/200..  Training Loss: 0.775..  Test Loss: 0.827..  Test Accuracy: 0.610\n",
      "Epoch: 68/200..  Training Loss: 0.773..  Test Loss: 0.825..  Test Accuracy: 0.611\n",
      "Epoch: 69/200..  Training Loss: 0.770..  Test Loss: 0.824..  Test Accuracy: 0.612\n",
      "Epoch: 70/200..  Training Loss: 0.768..  Test Loss: 0.822..  Test Accuracy: 0.612\n",
      "Epoch: 71/200..  Training Loss: 0.765..  Test Loss: 0.821..  Test Accuracy: 0.613\n",
      "Epoch: 72/200..  Training Loss: 0.762..  Test Loss: 0.820..  Test Accuracy: 0.614\n",
      "Epoch: 73/200..  Training Loss: 0.760..  Test Loss: 0.819..  Test Accuracy: 0.615\n",
      "Epoch: 74/200..  Training Loss: 0.758..  Test Loss: 0.818..  Test Accuracy: 0.615\n",
      "Epoch: 75/200..  Training Loss: 0.755..  Test Loss: 0.816..  Test Accuracy: 0.615\n",
      "Epoch: 76/200..  Training Loss: 0.754..  Test Loss: 0.815..  Test Accuracy: 0.616\n",
      "Epoch: 77/200..  Training Loss: 0.751..  Test Loss: 0.815..  Test Accuracy: 0.616\n",
      "Epoch: 78/200..  Training Loss: 0.750..  Test Loss: 0.814..  Test Accuracy: 0.616\n",
      "Epoch: 79/200..  Training Loss: 0.747..  Test Loss: 0.813..  Test Accuracy: 0.616\n",
      "Epoch: 80/200..  Training Loss: 0.745..  Test Loss: 0.812..  Test Accuracy: 0.616\n",
      "Epoch: 81/200..  Training Loss: 0.743..  Test Loss: 0.811..  Test Accuracy: 0.617\n",
      "Epoch: 82/200..  Training Loss: 0.743..  Test Loss: 0.810..  Test Accuracy: 0.618\n",
      "Epoch: 83/200..  Training Loss: 0.740..  Test Loss: 0.810..  Test Accuracy: 0.619\n",
      "Epoch: 84/200..  Training Loss: 0.739..  Test Loss: 0.809..  Test Accuracy: 0.619\n",
      "Epoch: 85/200..  Training Loss: 0.737..  Test Loss: 0.808..  Test Accuracy: 0.620\n",
      "Epoch: 86/200..  Training Loss: 0.735..  Test Loss: 0.808..  Test Accuracy: 0.621\n",
      "Epoch: 87/200..  Training Loss: 0.733..  Test Loss: 0.807..  Test Accuracy: 0.621\n",
      "Epoch: 88/200..  Training Loss: 0.732..  Test Loss: 0.806..  Test Accuracy: 0.621\n",
      "Epoch: 89/200..  Training Loss: 0.730..  Test Loss: 0.806..  Test Accuracy: 0.620\n",
      "Epoch: 90/200..  Training Loss: 0.729..  Test Loss: 0.805..  Test Accuracy: 0.620\n",
      "Epoch: 91/200..  Training Loss: 0.728..  Test Loss: 0.805..  Test Accuracy: 0.621\n",
      "Epoch: 92/200..  Training Loss: 0.726..  Test Loss: 0.804..  Test Accuracy: 0.621\n",
      "Epoch: 93/200..  Training Loss: 0.725..  Test Loss: 0.804..  Test Accuracy: 0.621\n",
      "Epoch: 94/200..  Training Loss: 0.723..  Test Loss: 0.803..  Test Accuracy: 0.622\n",
      "Epoch: 95/200..  Training Loss: 0.722..  Test Loss: 0.803..  Test Accuracy: 0.621\n",
      "Epoch: 96/200..  Training Loss: 0.721..  Test Loss: 0.802..  Test Accuracy: 0.622\n",
      "Epoch: 97/200..  Training Loss: 0.720..  Test Loss: 0.802..  Test Accuracy: 0.622\n",
      "Epoch: 98/200..  Training Loss: 0.719..  Test Loss: 0.802..  Test Accuracy: 0.623\n",
      "Epoch: 99/200..  Training Loss: 0.718..  Test Loss: 0.801..  Test Accuracy: 0.624\n",
      "Epoch: 100/200..  Training Loss: 0.717..  Test Loss: 0.801..  Test Accuracy: 0.625\n",
      "Epoch: 101/200..  Training Loss: 0.715..  Test Loss: 0.800..  Test Accuracy: 0.625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 102/200..  Training Loss: 0.714..  Test Loss: 0.800..  Test Accuracy: 0.626\n",
      "Epoch: 103/200..  Training Loss: 0.714..  Test Loss: 0.800..  Test Accuracy: 0.626\n",
      "Epoch: 104/200..  Training Loss: 0.712..  Test Loss: 0.799..  Test Accuracy: 0.627\n",
      "Epoch: 105/200..  Training Loss: 0.711..  Test Loss: 0.799..  Test Accuracy: 0.627\n",
      "Epoch: 106/200..  Training Loss: 0.710..  Test Loss: 0.799..  Test Accuracy: 0.628\n",
      "Epoch: 107/200..  Training Loss: 0.710..  Test Loss: 0.798..  Test Accuracy: 0.628\n",
      "Epoch: 108/200..  Training Loss: 0.709..  Test Loss: 0.798..  Test Accuracy: 0.628\n",
      "Epoch: 109/200..  Training Loss: 0.706..  Test Loss: 0.798..  Test Accuracy: 0.629\n",
      "Epoch: 110/200..  Training Loss: 0.706..  Test Loss: 0.798..  Test Accuracy: 0.629\n",
      "Epoch: 111/200..  Training Loss: 0.705..  Test Loss: 0.797..  Test Accuracy: 0.629\n",
      "Epoch: 112/200..  Training Loss: 0.705..  Test Loss: 0.797..  Test Accuracy: 0.629\n",
      "Epoch: 113/200..  Training Loss: 0.703..  Test Loss: 0.797..  Test Accuracy: 0.630\n",
      "Epoch: 114/200..  Training Loss: 0.703..  Test Loss: 0.797..  Test Accuracy: 0.630\n",
      "Epoch: 115/200..  Training Loss: 0.702..  Test Loss: 0.796..  Test Accuracy: 0.630\n",
      "Epoch: 116/200..  Training Loss: 0.701..  Test Loss: 0.796..  Test Accuracy: 0.630\n",
      "Epoch: 117/200..  Training Loss: 0.700..  Test Loss: 0.796..  Test Accuracy: 0.630\n",
      "Epoch: 118/200..  Training Loss: 0.700..  Test Loss: 0.796..  Test Accuracy: 0.630\n",
      "Epoch: 119/200..  Training Loss: 0.699..  Test Loss: 0.795..  Test Accuracy: 0.630\n",
      "Epoch: 120/200..  Training Loss: 0.698..  Test Loss: 0.795..  Test Accuracy: 0.630\n",
      "Epoch: 121/200..  Training Loss: 0.697..  Test Loss: 0.795..  Test Accuracy: 0.631\n",
      "Epoch: 122/200..  Training Loss: 0.696..  Test Loss: 0.795..  Test Accuracy: 0.632\n",
      "Epoch: 123/200..  Training Loss: 0.696..  Test Loss: 0.795..  Test Accuracy: 0.632\n",
      "Epoch: 124/200..  Training Loss: 0.696..  Test Loss: 0.795..  Test Accuracy: 0.632\n",
      "Epoch: 125/200..  Training Loss: 0.694..  Test Loss: 0.794..  Test Accuracy: 0.631\n",
      "Epoch: 126/200..  Training Loss: 0.693..  Test Loss: 0.794..  Test Accuracy: 0.631\n",
      "Epoch: 127/200..  Training Loss: 0.693..  Test Loss: 0.794..  Test Accuracy: 0.630\n",
      "Epoch: 128/200..  Training Loss: 0.692..  Test Loss: 0.794..  Test Accuracy: 0.631\n",
      "Epoch: 129/200..  Training Loss: 0.692..  Test Loss: 0.794..  Test Accuracy: 0.631\n",
      "Epoch: 130/200..  Training Loss: 0.691..  Test Loss: 0.794..  Test Accuracy: 0.631\n",
      "Epoch: 131/200..  Training Loss: 0.690..  Test Loss: 0.793..  Test Accuracy: 0.631\n",
      "Epoch: 132/200..  Training Loss: 0.690..  Test Loss: 0.793..  Test Accuracy: 0.631\n",
      "Epoch: 133/200..  Training Loss: 0.689..  Test Loss: 0.793..  Test Accuracy: 0.631\n",
      "Epoch: 134/200..  Training Loss: 0.688..  Test Loss: 0.793..  Test Accuracy: 0.631\n",
      "Epoch: 135/200..  Training Loss: 0.689..  Test Loss: 0.793..  Test Accuracy: 0.632\n",
      "Epoch: 136/200..  Training Loss: 0.687..  Test Loss: 0.793..  Test Accuracy: 0.633\n",
      "Epoch: 137/200..  Training Loss: 0.686..  Test Loss: 0.793..  Test Accuracy: 0.633\n",
      "Epoch: 138/200..  Training Loss: 0.686..  Test Loss: 0.792..  Test Accuracy: 0.633\n",
      "Epoch: 139/200..  Training Loss: 0.685..  Test Loss: 0.792..  Test Accuracy: 0.634\n",
      "Epoch: 140/200..  Training Loss: 0.684..  Test Loss: 0.792..  Test Accuracy: 0.634\n",
      "Epoch: 141/200..  Training Loss: 0.683..  Test Loss: 0.792..  Test Accuracy: 0.634\n",
      "Epoch: 142/200..  Training Loss: 0.683..  Test Loss: 0.792..  Test Accuracy: 0.634\n",
      "Epoch: 143/200..  Training Loss: 0.683..  Test Loss: 0.792..  Test Accuracy: 0.634\n",
      "Epoch: 144/200..  Training Loss: 0.681..  Test Loss: 0.792..  Test Accuracy: 0.634\n",
      "Epoch: 145/200..  Training Loss: 0.681..  Test Loss: 0.792..  Test Accuracy: 0.635\n",
      "Epoch: 146/200..  Training Loss: 0.680..  Test Loss: 0.792..  Test Accuracy: 0.635\n",
      "Epoch: 147/200..  Training Loss: 0.680..  Test Loss: 0.791..  Test Accuracy: 0.635\n",
      "Epoch: 148/200..  Training Loss: 0.680..  Test Loss: 0.791..  Test Accuracy: 0.635\n",
      "Epoch: 149/200..  Training Loss: 0.679..  Test Loss: 0.791..  Test Accuracy: 0.635\n",
      "Epoch: 150/200..  Training Loss: 0.678..  Test Loss: 0.791..  Test Accuracy: 0.635\n",
      "Epoch: 151/200..  Training Loss: 0.677..  Test Loss: 0.791..  Test Accuracy: 0.635\n",
      "Epoch: 152/200..  Training Loss: 0.676..  Test Loss: 0.791..  Test Accuracy: 0.635\n",
      "Epoch: 153/200..  Training Loss: 0.676..  Test Loss: 0.791..  Test Accuracy: 0.635\n",
      "Epoch: 154/200..  Training Loss: 0.675..  Test Loss: 0.791..  Test Accuracy: 0.635\n",
      "Epoch: 155/200..  Training Loss: 0.675..  Test Loss: 0.791..  Test Accuracy: 0.636\n",
      "Epoch: 156/200..  Training Loss: 0.674..  Test Loss: 0.791..  Test Accuracy: 0.636\n",
      "Epoch: 157/200..  Training Loss: 0.673..  Test Loss: 0.791..  Test Accuracy: 0.635\n",
      "Epoch: 158/200..  Training Loss: 0.673..  Test Loss: 0.791..  Test Accuracy: 0.635\n",
      "Epoch: 159/200..  Training Loss: 0.672..  Test Loss: 0.791..  Test Accuracy: 0.636\n",
      "Epoch: 160/200..  Training Loss: 0.672..  Test Loss: 0.791..  Test Accuracy: 0.636\n",
      "Epoch: 161/200..  Training Loss: 0.672..  Test Loss: 0.791..  Test Accuracy: 0.636\n",
      "Epoch: 162/200..  Training Loss: 0.670..  Test Loss: 0.791..  Test Accuracy: 0.636\n",
      "Epoch: 163/200..  Training Loss: 0.670..  Test Loss: 0.791..  Test Accuracy: 0.636\n",
      "Epoch: 164/200..  Training Loss: 0.670..  Test Loss: 0.791..  Test Accuracy: 0.637\n",
      "Epoch: 165/200..  Training Loss: 0.669..  Test Loss: 0.790..  Test Accuracy: 0.637\n",
      "Epoch: 166/200..  Training Loss: 0.668..  Test Loss: 0.790..  Test Accuracy: 0.637\n",
      "Epoch: 167/200..  Training Loss: 0.668..  Test Loss: 0.790..  Test Accuracy: 0.637\n",
      "Epoch: 168/200..  Training Loss: 0.668..  Test Loss: 0.790..  Test Accuracy: 0.638\n",
      "Epoch: 169/200..  Training Loss: 0.666..  Test Loss: 0.790..  Test Accuracy: 0.638\n",
      "Epoch: 170/200..  Training Loss: 0.665..  Test Loss: 0.790..  Test Accuracy: 0.638\n",
      "Epoch: 171/200..  Training Loss: 0.666..  Test Loss: 0.790..  Test Accuracy: 0.638\n",
      "Epoch: 172/200..  Training Loss: 0.664..  Test Loss: 0.790..  Test Accuracy: 0.638\n",
      "Epoch: 173/200..  Training Loss: 0.664..  Test Loss: 0.790..  Test Accuracy: 0.638\n",
      "Epoch: 174/200..  Training Loss: 0.663..  Test Loss: 0.790..  Test Accuracy: 0.638\n",
      "Epoch: 175/200..  Training Loss: 0.663..  Test Loss: 0.790..  Test Accuracy: 0.638\n",
      "Epoch: 176/200..  Training Loss: 0.662..  Test Loss: 0.790..  Test Accuracy: 0.638\n",
      "Epoch: 177/200..  Training Loss: 0.662..  Test Loss: 0.790..  Test Accuracy: 0.638\n",
      "Epoch: 178/200..  Training Loss: 0.661..  Test Loss: 0.790..  Test Accuracy: 0.637\n",
      "Epoch: 179/200..  Training Loss: 0.660..  Test Loss: 0.790..  Test Accuracy: 0.637\n",
      "Epoch: 180/200..  Training Loss: 0.659..  Test Loss: 0.790..  Test Accuracy: 0.637\n",
      "Epoch: 181/200..  Training Loss: 0.659..  Test Loss: 0.790..  Test Accuracy: 0.638\n",
      "Epoch: 182/200..  Training Loss: 0.658..  Test Loss: 0.790..  Test Accuracy: 0.638\n",
      "Epoch: 183/200..  Training Loss: 0.658..  Test Loss: 0.790..  Test Accuracy: 0.638\n",
      "Epoch: 184/200..  Training Loss: 0.657..  Test Loss: 0.791..  Test Accuracy: 0.638\n",
      "Epoch: 185/200..  Training Loss: 0.657..  Test Loss: 0.791..  Test Accuracy: 0.638\n",
      "Epoch: 186/200..  Training Loss: 0.656..  Test Loss: 0.791..  Test Accuracy: 0.639\n",
      "Epoch: 187/200..  Training Loss: 0.655..  Test Loss: 0.791..  Test Accuracy: 0.639\n",
      "Epoch: 188/200..  Training Loss: 0.654..  Test Loss: 0.791..  Test Accuracy: 0.639\n",
      "Epoch: 189/200..  Training Loss: 0.654..  Test Loss: 0.791..  Test Accuracy: 0.639\n",
      "Epoch: 190/200..  Training Loss: 0.652..  Test Loss: 0.791..  Test Accuracy: 0.640\n",
      "Epoch: 191/200..  Training Loss: 0.653..  Test Loss: 0.791..  Test Accuracy: 0.640\n",
      "Epoch: 192/200..  Training Loss: 0.652..  Test Loss: 0.791..  Test Accuracy: 0.640\n",
      "Epoch: 193/200..  Training Loss: 0.651..  Test Loss: 0.791..  Test Accuracy: 0.640\n",
      "Epoch: 194/200..  Training Loss: 0.651..  Test Loss: 0.791..  Test Accuracy: 0.640\n",
      "Epoch: 195/200..  Training Loss: 0.650..  Test Loss: 0.791..  Test Accuracy: 0.640\n",
      "Epoch: 196/200..  Training Loss: 0.649..  Test Loss: 0.791..  Test Accuracy: 0.640\n",
      "Epoch: 197/200..  Training Loss: 0.649..  Test Loss: 0.791..  Test Accuracy: 0.641\n",
      "Epoch: 198/200..  Training Loss: 0.648..  Test Loss: 0.791..  Test Accuracy: 0.641\n",
      "Epoch: 199/200..  Training Loss: 0.647..  Test Loss: 0.791..  Test Accuracy: 0.641\n",
      "Epoch: 200/200..  Training Loss: 0.647..  Test Loss: 0.791..  Test Accuracy: 0.641\n",
      "CPU times: total: 3min 28s\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "epochs = 200\n",
    "for e in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model.forward(x_train) #Forward pass, get the logits\n",
    "    loss = criterion(output, y_train) # Calculate the loss with the logits and the labels\n",
    "    loss.backward()\n",
    "    train_loss = loss.item()\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    # Turn off gradients for validation, saves memory and computations\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        log_ps = model.forward(x_test)\n",
    "        test_loss = criterion(log_ps, y_test)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        ps = torch.exp(log_ps)\n",
    "        top_p, top_class  = ps.topk(1, dim=1)\n",
    "        equals = top_class == y_test.view(*top_class.shape)\n",
    "        test_accuracy = torch.mean(equals.float())\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    print(f\"Epoch: {e+1}/{epochs}.. \",\n",
    "          f\"Training Loss: {train_loss:.3f}.. \",\n",
    "          f\"Test Loss: {test_loss:.3f}.. \",\n",
    "          f\"Test Accuracy: {test_accuracy:.3f}\")\n",
    "#     scheduler.step(test_loss/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e88e68cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.forward(x_test)\n",
    "preds = topk_encoding(preds)\n",
    "preds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8eeb002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6362355309511827"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_score = accuracy_score(y_test, preds)\n",
    "acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90b29486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5975424613807815"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score = f1_score(y_test, preds, average='macro')\n",
    "f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a776b55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_vectorizer(message):\n",
    "    \"\"\"\n",
    "    Function to predict the category of inputted message\n",
    "    \"\"\"\n",
    "    \n",
    "    vec = vectorizer.transform(pd.Series(message))\n",
    "    vec = torch.tensor(scipy.sparse.csr_matrix.todense(vec)).float()\n",
    "    preds = model_1.forward(vec)\n",
    "    category = topk_encoding(preds).detach().cpu().numpy()\n",
    "    \n",
    "    return int(category[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e154e052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = input_vectorizer(\"do not be afraid to be saint be open\")\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3250737a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
