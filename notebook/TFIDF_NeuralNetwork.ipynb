{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26932adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "import joblib\n",
    "from joblib import load\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2baa8e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4bab239",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('twitter_cleaned.csv')\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1777c3",
   "metadata": {},
   "source": [
    "### Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92df9321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lower(message):\n",
    "    result = message.lower()\n",
    "    return result\n",
    "\n",
    "def remove_num(message):\n",
    "    result = re.sub(r'\\d+','',message)\n",
    "    return result\n",
    "\n",
    "def contractions(message):\n",
    "     result = re.sub(r\"won't\", \"will not\",message)\n",
    "     result = re.sub(r\"would't\", \"would not\",message)\n",
    "     result = re.sub(r\"could't\", \"could not\",message)\n",
    "     result = re.sub(r\"\\'d\", \" would\",message)\n",
    "     result = re.sub(r\"can\\'t\", \"can not\",message)\n",
    "     result = re.sub(r\"n\\'t\", \" not\", message)\n",
    "     result = re.sub(r\"\\'re\", \" are\", message)\n",
    "     result = re.sub(r\"\\'s\", \" is\", message)\n",
    "     result = re.sub(r\"\\'ll\", \" will\", message)\n",
    "     result = re.sub(r\"\\'t\", \" not\", message)\n",
    "     result = re.sub(r\"\\'ve\", \" have\", message)\n",
    "     result = re.sub(r\"\\'m\", \" am\", message)\n",
    "     return result\n",
    "    \n",
    "def remove_punctuation(message):\n",
    "    result = message.translate(str.maketrans(dict.fromkeys(string.punctuation)))\n",
    "    return result\n",
    "\n",
    "def remove_whitespace(message):\n",
    "    result = message.strip()\n",
    "    result = re.sub(' +',' ',message)\n",
    "    return result\n",
    "\n",
    "def replace_newline(message):\n",
    "    result = message.replace('\\n','')\n",
    "    return result\n",
    "\n",
    "def data_cleanup(message):\n",
    "    cleaning_utils = [to_lower, remove_num, contractions, remove_punctuation, remove_whitespace, replace_newline]\n",
    "    for util in cleaning_utils:\n",
    "        message = util(message)\n",
    "    return message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aeae90a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>prenatal move to wednesday at pm starting toni...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>happy nd birthday to prince george i cant beli...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>do not be afraid to be saint be open to the lo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neutral</td>\n",
       "      <td>dst is saturday nightsunday morning got ta be ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>sony reward app is like a lot of yo female sin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                            message  category\n",
       "0   neutral  prenatal move to wednesday at pm starting toni...         1\n",
       "1  positive  happy nd birthday to prince george i cant beli...         2\n",
       "2  positive  do not be afraid to be saint be open to the lo...         2\n",
       "3   neutral  dst is saturday nightsunday morning got ta be ...         1\n",
       "4  negative  sony reward app is like a lot of yo female sin...         0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1b1562a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39740,) (9935,) (39740,) (9935,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df['message'], df['category'], test_size=.2, stratify=df['label'])\n",
    "\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ff8634",
   "metadata": {},
   "source": [
    "### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "516af63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=2000)\n",
    "# vectorizer = TfidfVectorizer(max_features=1500, stop_words='english')\n",
    "\n",
    "# Learn vocabulary from training texts and vectorize training texts.\n",
    "x_train = vectorizer.fit_transform(x_train)\n",
    "\n",
    "# Vectorize test texts.\n",
    "x_test = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "349e550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(scipy.sparse.csr_matrix.todense(x_train)).float()\n",
    "x_test = torch.tensor(scipy.sparse.csr_matrix.todense(x_test)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ad06c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(y_train.values)\n",
    "y_test = torch.tensor(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54866aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_encoding(nd_array):\n",
    "    \"\"\"\n",
    "    Function to flatten the predicted category\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = nd_array\n",
    "    \n",
    "    ps = torch.exp(predictions)\n",
    "    top_p, top_class  = ps.topk(1, dim=1)\n",
    "    \n",
    "\n",
    "    return top_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfa9fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.hidden_layer_1 = nn.Linear(x_train.shape[1], 64) # input to first hidden layer\n",
    "        self.output_layer = nn.Linear(64, self.out_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.hidden_layer_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        y = self.output_layer(x)\n",
    "        y = self.activation(y)\n",
    "        y = self.softmax(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "371e2c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(x_train.shape[1], df['category'].nunique())\n",
    "# model = NeuralNetwork(x_train.shape[1], 5)\n",
    "\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "\n",
    "#setting up scheduler\n",
    "# scheduler = ReduceLROnPlateau(optimizer, 'min', patience = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f98d30",
   "metadata": {},
   "source": [
    "### Model Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6f106f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200..  Training Loss: 1.096..  Test Loss: 1.092..  Test Accuracy: 0.432\n",
      "Epoch: 2/200..  Training Loss: 1.091..  Test Loss: 1.088..  Test Accuracy: 0.537\n",
      "Epoch: 3/200..  Training Loss: 1.087..  Test Loss: 1.084..  Test Accuracy: 0.567\n",
      "Epoch: 4/200..  Training Loss: 1.083..  Test Loss: 1.079..  Test Accuracy: 0.567\n",
      "Epoch: 5/200..  Training Loss: 1.079..  Test Loss: 1.075..  Test Accuracy: 0.567\n",
      "Epoch: 6/200..  Training Loss: 1.074..  Test Loss: 1.071..  Test Accuracy: 0.567\n",
      "Epoch: 7/200..  Training Loss: 1.070..  Test Loss: 1.066..  Test Accuracy: 0.566\n",
      "Epoch: 8/200..  Training Loss: 1.065..  Test Loss: 1.061..  Test Accuracy: 0.569\n",
      "Epoch: 9/200..  Training Loss: 1.060..  Test Loss: 1.056..  Test Accuracy: 0.572\n",
      "Epoch: 10/200..  Training Loss: 1.054..  Test Loss: 1.051..  Test Accuracy: 0.572\n",
      "Epoch: 11/200..  Training Loss: 1.049..  Test Loss: 1.046..  Test Accuracy: 0.573\n",
      "Epoch: 12/200..  Training Loss: 1.044..  Test Loss: 1.041..  Test Accuracy: 0.575\n",
      "Epoch: 13/200..  Training Loss: 1.038..  Test Loss: 1.035..  Test Accuracy: 0.576\n",
      "Epoch: 14/200..  Training Loss: 1.032..  Test Loss: 1.030..  Test Accuracy: 0.576\n",
      "Epoch: 15/200..  Training Loss: 1.026..  Test Loss: 1.025..  Test Accuracy: 0.575\n",
      "Epoch: 16/200..  Training Loss: 1.021..  Test Loss: 1.019..  Test Accuracy: 0.578\n",
      "Epoch: 17/200..  Training Loss: 1.015..  Test Loss: 1.014..  Test Accuracy: 0.578\n",
      "Epoch: 18/200..  Training Loss: 1.009..  Test Loss: 1.008..  Test Accuracy: 0.579\n",
      "Epoch: 19/200..  Training Loss: 1.003..  Test Loss: 1.003..  Test Accuracy: 0.578\n",
      "Epoch: 20/200..  Training Loss: 0.997..  Test Loss: 0.997..  Test Accuracy: 0.579\n",
      "Epoch: 21/200..  Training Loss: 0.991..  Test Loss: 0.992..  Test Accuracy: 0.580\n",
      "Epoch: 22/200..  Training Loss: 0.985..  Test Loss: 0.986..  Test Accuracy: 0.581\n",
      "Epoch: 23/200..  Training Loss: 0.979..  Test Loss: 0.981..  Test Accuracy: 0.580\n",
      "Epoch: 24/200..  Training Loss: 0.974..  Test Loss: 0.976..  Test Accuracy: 0.581\n",
      "Epoch: 25/200..  Training Loss: 0.968..  Test Loss: 0.971..  Test Accuracy: 0.582\n",
      "Epoch: 26/200..  Training Loss: 0.962..  Test Loss: 0.965..  Test Accuracy: 0.582\n",
      "Epoch: 27/200..  Training Loss: 0.956..  Test Loss: 0.961..  Test Accuracy: 0.582\n",
      "Epoch: 28/200..  Training Loss: 0.951..  Test Loss: 0.956..  Test Accuracy: 0.583\n",
      "Epoch: 29/200..  Training Loss: 0.945..  Test Loss: 0.951..  Test Accuracy: 0.582\n",
      "Epoch: 30/200..  Training Loss: 0.940..  Test Loss: 0.946..  Test Accuracy: 0.581\n",
      "Epoch: 31/200..  Training Loss: 0.935..  Test Loss: 0.942..  Test Accuracy: 0.582\n",
      "Epoch: 32/200..  Training Loss: 0.929..  Test Loss: 0.938..  Test Accuracy: 0.583\n",
      "Epoch: 33/200..  Training Loss: 0.924..  Test Loss: 0.933..  Test Accuracy: 0.583\n",
      "Epoch: 34/200..  Training Loss: 0.920..  Test Loss: 0.929..  Test Accuracy: 0.584\n",
      "Epoch: 35/200..  Training Loss: 0.915..  Test Loss: 0.926..  Test Accuracy: 0.583\n",
      "Epoch: 36/200..  Training Loss: 0.910..  Test Loss: 0.922..  Test Accuracy: 0.583\n",
      "Epoch: 37/200..  Training Loss: 0.905..  Test Loss: 0.918..  Test Accuracy: 0.583\n",
      "Epoch: 38/200..  Training Loss: 0.901..  Test Loss: 0.915..  Test Accuracy: 0.584\n",
      "Epoch: 39/200..  Training Loss: 0.896..  Test Loss: 0.912..  Test Accuracy: 0.584\n",
      "Epoch: 40/200..  Training Loss: 0.892..  Test Loss: 0.908..  Test Accuracy: 0.584\n",
      "Epoch: 41/200..  Training Loss: 0.888..  Test Loss: 0.905..  Test Accuracy: 0.584\n",
      "Epoch: 42/200..  Training Loss: 0.885..  Test Loss: 0.902..  Test Accuracy: 0.584\n",
      "Epoch: 43/200..  Training Loss: 0.880..  Test Loss: 0.900..  Test Accuracy: 0.585\n",
      "Epoch: 44/200..  Training Loss: 0.877..  Test Loss: 0.897..  Test Accuracy: 0.585\n",
      "Epoch: 45/200..  Training Loss: 0.873..  Test Loss: 0.894..  Test Accuracy: 0.585\n",
      "Epoch: 46/200..  Training Loss: 0.870..  Test Loss: 0.892..  Test Accuracy: 0.586\n",
      "Epoch: 47/200..  Training Loss: 0.866..  Test Loss: 0.890..  Test Accuracy: 0.586\n",
      "Epoch: 48/200..  Training Loss: 0.863..  Test Loss: 0.887..  Test Accuracy: 0.587\n",
      "Epoch: 49/200..  Training Loss: 0.859..  Test Loss: 0.885..  Test Accuracy: 0.587\n",
      "Epoch: 50/200..  Training Loss: 0.856..  Test Loss: 0.883..  Test Accuracy: 0.587\n",
      "Epoch: 51/200..  Training Loss: 0.853..  Test Loss: 0.881..  Test Accuracy: 0.588\n",
      "Epoch: 52/200..  Training Loss: 0.850..  Test Loss: 0.879..  Test Accuracy: 0.588\n",
      "Epoch: 53/200..  Training Loss: 0.848..  Test Loss: 0.877..  Test Accuracy: 0.590\n",
      "Epoch: 54/200..  Training Loss: 0.845..  Test Loss: 0.875..  Test Accuracy: 0.589\n",
      "Epoch: 55/200..  Training Loss: 0.842..  Test Loss: 0.873..  Test Accuracy: 0.590\n",
      "Epoch: 56/200..  Training Loss: 0.839..  Test Loss: 0.871..  Test Accuracy: 0.590\n",
      "Epoch: 57/200..  Training Loss: 0.837..  Test Loss: 0.870..  Test Accuracy: 0.591\n",
      "Epoch: 58/200..  Training Loss: 0.834..  Test Loss: 0.868..  Test Accuracy: 0.590\n",
      "Epoch: 59/200..  Training Loss: 0.832..  Test Loss: 0.866..  Test Accuracy: 0.591\n",
      "Epoch: 60/200..  Training Loss: 0.830..  Test Loss: 0.865..  Test Accuracy: 0.593\n",
      "Epoch: 61/200..  Training Loss: 0.827..  Test Loss: 0.863..  Test Accuracy: 0.593\n",
      "Epoch: 62/200..  Training Loss: 0.824..  Test Loss: 0.862..  Test Accuracy: 0.593\n",
      "Epoch: 63/200..  Training Loss: 0.823..  Test Loss: 0.860..  Test Accuracy: 0.595\n",
      "Epoch: 64/200..  Training Loss: 0.820..  Test Loss: 0.859..  Test Accuracy: 0.596\n",
      "Epoch: 65/200..  Training Loss: 0.818..  Test Loss: 0.857..  Test Accuracy: 0.597\n",
      "Epoch: 66/200..  Training Loss: 0.816..  Test Loss: 0.856..  Test Accuracy: 0.599\n",
      "Epoch: 67/200..  Training Loss: 0.814..  Test Loss: 0.855..  Test Accuracy: 0.600\n",
      "Epoch: 68/200..  Training Loss: 0.813..  Test Loss: 0.854..  Test Accuracy: 0.600\n",
      "Epoch: 69/200..  Training Loss: 0.810..  Test Loss: 0.852..  Test Accuracy: 0.602\n",
      "Epoch: 70/200..  Training Loss: 0.808..  Test Loss: 0.851..  Test Accuracy: 0.603\n",
      "Epoch: 71/200..  Training Loss: 0.807..  Test Loss: 0.850..  Test Accuracy: 0.604\n",
      "Epoch: 72/200..  Training Loss: 0.805..  Test Loss: 0.849..  Test Accuracy: 0.604\n",
      "Epoch: 73/200..  Training Loss: 0.803..  Test Loss: 0.848..  Test Accuracy: 0.605\n",
      "Epoch: 74/200..  Training Loss: 0.801..  Test Loss: 0.847..  Test Accuracy: 0.605\n",
      "Epoch: 75/200..  Training Loss: 0.800..  Test Loss: 0.845..  Test Accuracy: 0.606\n",
      "Epoch: 76/200..  Training Loss: 0.798..  Test Loss: 0.844..  Test Accuracy: 0.607\n",
      "Epoch: 77/200..  Training Loss: 0.796..  Test Loss: 0.843..  Test Accuracy: 0.607\n",
      "Epoch: 78/200..  Training Loss: 0.795..  Test Loss: 0.842..  Test Accuracy: 0.609\n",
      "Epoch: 79/200..  Training Loss: 0.793..  Test Loss: 0.841..  Test Accuracy: 0.609\n",
      "Epoch: 80/200..  Training Loss: 0.792..  Test Loss: 0.841..  Test Accuracy: 0.610\n",
      "Epoch: 81/200..  Training Loss: 0.790..  Test Loss: 0.840..  Test Accuracy: 0.611\n",
      "Epoch: 82/200..  Training Loss: 0.789..  Test Loss: 0.839..  Test Accuracy: 0.611\n",
      "Epoch: 83/200..  Training Loss: 0.788..  Test Loss: 0.838..  Test Accuracy: 0.611\n",
      "Epoch: 84/200..  Training Loss: 0.786..  Test Loss: 0.837..  Test Accuracy: 0.611\n",
      "Epoch: 85/200..  Training Loss: 0.785..  Test Loss: 0.836..  Test Accuracy: 0.612\n",
      "Epoch: 86/200..  Training Loss: 0.784..  Test Loss: 0.835..  Test Accuracy: 0.614\n",
      "Epoch: 87/200..  Training Loss: 0.782..  Test Loss: 0.835..  Test Accuracy: 0.615\n",
      "Epoch: 88/200..  Training Loss: 0.781..  Test Loss: 0.834..  Test Accuracy: 0.614\n",
      "Epoch: 89/200..  Training Loss: 0.779..  Test Loss: 0.833..  Test Accuracy: 0.615\n",
      "Epoch: 90/200..  Training Loss: 0.778..  Test Loss: 0.833..  Test Accuracy: 0.615\n",
      "Epoch: 91/200..  Training Loss: 0.777..  Test Loss: 0.832..  Test Accuracy: 0.615\n",
      "Epoch: 92/200..  Training Loss: 0.776..  Test Loss: 0.831..  Test Accuracy: 0.615\n",
      "Epoch: 93/200..  Training Loss: 0.775..  Test Loss: 0.831..  Test Accuracy: 0.615\n",
      "Epoch: 94/200..  Training Loss: 0.774..  Test Loss: 0.830..  Test Accuracy: 0.615\n",
      "Epoch: 95/200..  Training Loss: 0.772..  Test Loss: 0.829..  Test Accuracy: 0.616\n",
      "Epoch: 96/200..  Training Loss: 0.771..  Test Loss: 0.829..  Test Accuracy: 0.617\n",
      "Epoch: 97/200..  Training Loss: 0.770..  Test Loss: 0.828..  Test Accuracy: 0.617\n",
      "Epoch: 98/200..  Training Loss: 0.770..  Test Loss: 0.828..  Test Accuracy: 0.617\n",
      "Epoch: 99/200..  Training Loss: 0.769..  Test Loss: 0.827..  Test Accuracy: 0.618\n",
      "Epoch: 100/200..  Training Loss: 0.767..  Test Loss: 0.827..  Test Accuracy: 0.617\n",
      "Epoch: 101/200..  Training Loss: 0.767..  Test Loss: 0.826..  Test Accuracy: 0.618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 102/200..  Training Loss: 0.765..  Test Loss: 0.826..  Test Accuracy: 0.618\n",
      "Epoch: 103/200..  Training Loss: 0.764..  Test Loss: 0.825..  Test Accuracy: 0.618\n",
      "Epoch: 104/200..  Training Loss: 0.763..  Test Loss: 0.825..  Test Accuracy: 0.618\n",
      "Epoch: 105/200..  Training Loss: 0.762..  Test Loss: 0.824..  Test Accuracy: 0.620\n",
      "Epoch: 106/200..  Training Loss: 0.762..  Test Loss: 0.824..  Test Accuracy: 0.620\n",
      "Epoch: 107/200..  Training Loss: 0.761..  Test Loss: 0.824..  Test Accuracy: 0.620\n",
      "Epoch: 108/200..  Training Loss: 0.760..  Test Loss: 0.823..  Test Accuracy: 0.620\n",
      "Epoch: 109/200..  Training Loss: 0.759..  Test Loss: 0.823..  Test Accuracy: 0.620\n",
      "Epoch: 110/200..  Training Loss: 0.758..  Test Loss: 0.822..  Test Accuracy: 0.619\n",
      "Epoch: 111/200..  Training Loss: 0.757..  Test Loss: 0.822..  Test Accuracy: 0.620\n",
      "Epoch: 112/200..  Training Loss: 0.757..  Test Loss: 0.822..  Test Accuracy: 0.621\n",
      "Epoch: 113/200..  Training Loss: 0.755..  Test Loss: 0.821..  Test Accuracy: 0.621\n",
      "Epoch: 114/200..  Training Loss: 0.755..  Test Loss: 0.821..  Test Accuracy: 0.622\n",
      "Epoch: 115/200..  Training Loss: 0.754..  Test Loss: 0.821..  Test Accuracy: 0.622\n",
      "Epoch: 116/200..  Training Loss: 0.753..  Test Loss: 0.820..  Test Accuracy: 0.622\n",
      "Epoch: 117/200..  Training Loss: 0.752..  Test Loss: 0.820..  Test Accuracy: 0.622\n",
      "Epoch: 118/200..  Training Loss: 0.751..  Test Loss: 0.820..  Test Accuracy: 0.621\n",
      "Epoch: 119/200..  Training Loss: 0.751..  Test Loss: 0.820..  Test Accuracy: 0.622\n",
      "Epoch: 120/200..  Training Loss: 0.751..  Test Loss: 0.819..  Test Accuracy: 0.622\n",
      "Epoch: 121/200..  Training Loss: 0.750..  Test Loss: 0.819..  Test Accuracy: 0.622\n",
      "Epoch: 122/200..  Training Loss: 0.749..  Test Loss: 0.819..  Test Accuracy: 0.622\n",
      "Epoch: 123/200..  Training Loss: 0.747..  Test Loss: 0.818..  Test Accuracy: 0.622\n",
      "Epoch: 124/200..  Training Loss: 0.747..  Test Loss: 0.818..  Test Accuracy: 0.623\n",
      "Epoch: 125/200..  Training Loss: 0.746..  Test Loss: 0.818..  Test Accuracy: 0.623\n",
      "Epoch: 126/200..  Training Loss: 0.746..  Test Loss: 0.818..  Test Accuracy: 0.623\n",
      "Epoch: 127/200..  Training Loss: 0.745..  Test Loss: 0.817..  Test Accuracy: 0.623\n",
      "Epoch: 128/200..  Training Loss: 0.744..  Test Loss: 0.817..  Test Accuracy: 0.624\n",
      "Epoch: 129/200..  Training Loss: 0.743..  Test Loss: 0.817..  Test Accuracy: 0.624\n",
      "Epoch: 130/200..  Training Loss: 0.743..  Test Loss: 0.817..  Test Accuracy: 0.624\n",
      "Epoch: 131/200..  Training Loss: 0.742..  Test Loss: 0.817..  Test Accuracy: 0.624\n",
      "Epoch: 132/200..  Training Loss: 0.741..  Test Loss: 0.816..  Test Accuracy: 0.624\n",
      "Epoch: 133/200..  Training Loss: 0.741..  Test Loss: 0.816..  Test Accuracy: 0.624\n",
      "Epoch: 134/200..  Training Loss: 0.740..  Test Loss: 0.816..  Test Accuracy: 0.625\n",
      "Epoch: 135/200..  Training Loss: 0.739..  Test Loss: 0.816..  Test Accuracy: 0.625\n",
      "Epoch: 136/200..  Training Loss: 0.739..  Test Loss: 0.816..  Test Accuracy: 0.625\n",
      "Epoch: 137/200..  Training Loss: 0.738..  Test Loss: 0.816..  Test Accuracy: 0.626\n",
      "Epoch: 138/200..  Training Loss: 0.737..  Test Loss: 0.815..  Test Accuracy: 0.626\n",
      "Epoch: 139/200..  Training Loss: 0.737..  Test Loss: 0.815..  Test Accuracy: 0.626\n",
      "Epoch: 140/200..  Training Loss: 0.737..  Test Loss: 0.815..  Test Accuracy: 0.626\n",
      "Epoch: 141/200..  Training Loss: 0.736..  Test Loss: 0.815..  Test Accuracy: 0.626\n",
      "Epoch: 142/200..  Training Loss: 0.735..  Test Loss: 0.815..  Test Accuracy: 0.627\n",
      "Epoch: 143/200..  Training Loss: 0.734..  Test Loss: 0.815..  Test Accuracy: 0.627\n",
      "Epoch: 144/200..  Training Loss: 0.733..  Test Loss: 0.815..  Test Accuracy: 0.627\n",
      "Epoch: 145/200..  Training Loss: 0.733..  Test Loss: 0.814..  Test Accuracy: 0.627\n",
      "Epoch: 146/200..  Training Loss: 0.732..  Test Loss: 0.814..  Test Accuracy: 0.629\n",
      "Epoch: 147/200..  Training Loss: 0.732..  Test Loss: 0.814..  Test Accuracy: 0.629\n",
      "Epoch: 148/200..  Training Loss: 0.731..  Test Loss: 0.814..  Test Accuracy: 0.629\n",
      "Epoch: 149/200..  Training Loss: 0.731..  Test Loss: 0.814..  Test Accuracy: 0.629\n",
      "Epoch: 150/200..  Training Loss: 0.730..  Test Loss: 0.814..  Test Accuracy: 0.629\n",
      "Epoch: 151/200..  Training Loss: 0.730..  Test Loss: 0.814..  Test Accuracy: 0.629\n",
      "Epoch: 152/200..  Training Loss: 0.729..  Test Loss: 0.814..  Test Accuracy: 0.629\n",
      "Epoch: 153/200..  Training Loss: 0.728..  Test Loss: 0.814..  Test Accuracy: 0.629\n",
      "Epoch: 154/200..  Training Loss: 0.728..  Test Loss: 0.813..  Test Accuracy: 0.629\n",
      "Epoch: 155/200..  Training Loss: 0.728..  Test Loss: 0.813..  Test Accuracy: 0.629\n",
      "Epoch: 156/200..  Training Loss: 0.728..  Test Loss: 0.813..  Test Accuracy: 0.628\n",
      "Epoch: 157/200..  Training Loss: 0.726..  Test Loss: 0.813..  Test Accuracy: 0.628\n",
      "Epoch: 158/200..  Training Loss: 0.726..  Test Loss: 0.813..  Test Accuracy: 0.628\n",
      "Epoch: 159/200..  Training Loss: 0.725..  Test Loss: 0.813..  Test Accuracy: 0.628\n",
      "Epoch: 160/200..  Training Loss: 0.725..  Test Loss: 0.813..  Test Accuracy: 0.628\n",
      "Epoch: 161/200..  Training Loss: 0.725..  Test Loss: 0.813..  Test Accuracy: 0.629\n",
      "Epoch: 162/200..  Training Loss: 0.723..  Test Loss: 0.813..  Test Accuracy: 0.629\n",
      "Epoch: 163/200..  Training Loss: 0.723..  Test Loss: 0.813..  Test Accuracy: 0.629\n",
      "Epoch: 164/200..  Training Loss: 0.722..  Test Loss: 0.812..  Test Accuracy: 0.629\n",
      "Epoch: 165/200..  Training Loss: 0.722..  Test Loss: 0.812..  Test Accuracy: 0.629\n",
      "Epoch: 166/200..  Training Loss: 0.721..  Test Loss: 0.812..  Test Accuracy: 0.630\n",
      "Epoch: 167/200..  Training Loss: 0.721..  Test Loss: 0.812..  Test Accuracy: 0.630\n",
      "Epoch: 168/200..  Training Loss: 0.720..  Test Loss: 0.812..  Test Accuracy: 0.630\n",
      "Epoch: 169/200..  Training Loss: 0.720..  Test Loss: 0.812..  Test Accuracy: 0.630\n",
      "Epoch: 170/200..  Training Loss: 0.720..  Test Loss: 0.812..  Test Accuracy: 0.631\n",
      "Epoch: 171/200..  Training Loss: 0.718..  Test Loss: 0.812..  Test Accuracy: 0.631\n",
      "Epoch: 172/200..  Training Loss: 0.718..  Test Loss: 0.812..  Test Accuracy: 0.630\n",
      "Epoch: 173/200..  Training Loss: 0.718..  Test Loss: 0.812..  Test Accuracy: 0.631\n",
      "Epoch: 174/200..  Training Loss: 0.717..  Test Loss: 0.812..  Test Accuracy: 0.631\n",
      "Epoch: 175/200..  Training Loss: 0.716..  Test Loss: 0.812..  Test Accuracy: 0.632\n",
      "Epoch: 176/200..  Training Loss: 0.716..  Test Loss: 0.812..  Test Accuracy: 0.633\n",
      "Epoch: 177/200..  Training Loss: 0.715..  Test Loss: 0.812..  Test Accuracy: 0.633\n",
      "Epoch: 178/200..  Training Loss: 0.715..  Test Loss: 0.812..  Test Accuracy: 0.633\n",
      "Epoch: 179/200..  Training Loss: 0.715..  Test Loss: 0.812..  Test Accuracy: 0.633\n",
      "Epoch: 180/200..  Training Loss: 0.714..  Test Loss: 0.812..  Test Accuracy: 0.633\n",
      "Epoch: 181/200..  Training Loss: 0.713..  Test Loss: 0.812..  Test Accuracy: 0.633\n",
      "Epoch: 182/200..  Training Loss: 0.713..  Test Loss: 0.812..  Test Accuracy: 0.633\n",
      "Epoch: 183/200..  Training Loss: 0.712..  Test Loss: 0.812..  Test Accuracy: 0.633\n",
      "Epoch: 184/200..  Training Loss: 0.711..  Test Loss: 0.811..  Test Accuracy: 0.633\n",
      "Epoch: 185/200..  Training Loss: 0.711..  Test Loss: 0.811..  Test Accuracy: 0.633\n",
      "Epoch: 186/200..  Training Loss: 0.711..  Test Loss: 0.811..  Test Accuracy: 0.634\n",
      "Epoch: 187/200..  Training Loss: 0.710..  Test Loss: 0.811..  Test Accuracy: 0.634\n",
      "Epoch: 188/200..  Training Loss: 0.709..  Test Loss: 0.811..  Test Accuracy: 0.634\n",
      "Epoch: 189/200..  Training Loss: 0.709..  Test Loss: 0.811..  Test Accuracy: 0.633\n",
      "Epoch: 190/200..  Training Loss: 0.709..  Test Loss: 0.811..  Test Accuracy: 0.634\n",
      "Epoch: 191/200..  Training Loss: 0.708..  Test Loss: 0.811..  Test Accuracy: 0.633\n",
      "Epoch: 192/200..  Training Loss: 0.707..  Test Loss: 0.811..  Test Accuracy: 0.633\n",
      "Epoch: 193/200..  Training Loss: 0.707..  Test Loss: 0.811..  Test Accuracy: 0.633\n",
      "Epoch: 194/200..  Training Loss: 0.706..  Test Loss: 0.811..  Test Accuracy: 0.633\n",
      "Epoch: 195/200..  Training Loss: 0.706..  Test Loss: 0.811..  Test Accuracy: 0.633\n",
      "Epoch: 196/200..  Training Loss: 0.705..  Test Loss: 0.811..  Test Accuracy: 0.633\n",
      "Epoch: 197/200..  Training Loss: 0.704..  Test Loss: 0.811..  Test Accuracy: 0.633\n",
      "Epoch: 198/200..  Training Loss: 0.704..  Test Loss: 0.811..  Test Accuracy: 0.633\n",
      "Epoch: 199/200..  Training Loss: 0.703..  Test Loss: 0.811..  Test Accuracy: 0.633\n",
      "Epoch: 200/200..  Training Loss: 0.703..  Test Loss: 0.811..  Test Accuracy: 0.633\n",
      "CPU times: total: 2min 12s\n",
      "Wall time: 33.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "epochs = 200\n",
    "for e in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model.forward(x_train) #Forward pass, get the logits\n",
    "    loss = criterion(output, y_train) # Calculate the loss with the logits and the labels\n",
    "    loss.backward()\n",
    "    train_loss = loss.item()\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    # Turn off gradients for validation, saves memory and computations\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        log_ps = model.forward(x_test)\n",
    "        test_loss = criterion(log_ps, y_test)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        ps = torch.exp(log_ps)\n",
    "        top_p, top_class  = ps.topk(1, dim=1)\n",
    "        equals = top_class == y_test.view(*top_class.shape)\n",
    "        test_accuracy = torch.mean(equals.float())\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    print(f\"Epoch: {e+1}/{epochs}.. \",\n",
    "          f\"Training Loss: {train_loss:.3f}.. \",\n",
    "          f\"Test Loss: {test_loss:.3f}.. \",\n",
    "          f\"Test Accuracy: {test_accuracy:.3f}\")\n",
    "#     scheduler.step(test_loss/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e88e68cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.forward(x_test)\n",
    "preds = topk_encoding(preds)\n",
    "preds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8eeb002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6331152491192753"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_score = accuracy_score(y_test, preds)\n",
    "acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90b29486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5659331873405122"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score = f1_score(y_test, preds, average='macro')\n",
    "f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb94caaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model/tfidf_vectorizer.joblib\", \"wb+\") as filename:\n",
    "        joblib.dump(vectorizer, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "409b3342",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model/nn_model.joblib\", \"wb+\") as filename:\n",
    "        joblib.dump(model, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017dfec9",
   "metadata": {},
   "source": [
    "### ML Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc4ef20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_vectorizer(message):\n",
    "    \"\"\"\n",
    "    Function to predict the category of inputted message\n",
    "    \"\"\"\n",
    "    \n",
    "    cleaned_message = pd.Series(message).apply(lambda x: data_cleanup(x))\n",
    "    vec = vectorizer.transform(pd.Series(cleaned_message))\n",
    "    vec = torch.tensor(scipy.sparse.csr_matrix.todense(vec)).float()\n",
    "    preds = model.forward(vec)\n",
    "    category = topk_encoding(preds).detach().cpu().numpy()\n",
    "    \n",
    "    return int(category[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9df74696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = input_vectorizer(\"fly to the sky to go back after a year on september.\")\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2722396e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = input_vectorizer(\"the item is not good, waste of money\")\n",
    "b\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
